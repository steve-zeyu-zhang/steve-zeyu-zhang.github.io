
<!DYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="icon" type="image/x-icon" href="image/zzhang.svg">
  <title>Zeyu Zhang</title>
  
  <meta name="author" content="Zeyu Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <style>
    .spacer {
        height: 5px;
    }
</style>
<style>
  .spacer2 {
      height: 1px;
  }
</style>
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="height: 5px;">&nbsp;</p><br>
              <p style="text-align:center">
                <name> Zeyu Zhang </name>
              </p>
              <p> Zeyu Zhang is an undergraduate researcher under the guidance of <a href="http://users.cecs.anu.edu.au/~hartley/">Prof. Richard Hartley</a> and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>. His research interests are rooted in computer vision, focusing on generative 3D modeling and AI for health. Specifically, he is dedicated to advancing efficient and high-quality motion and avatar generation, as well as 3D medical imaging segmentation and representation learning. With extensive experience across multiple research disciplines, Zeyu actively explores cutting-edge advancements in both the foundational and applied aspects of artificial intelligence. He has also collaborated closely with <a href="https://ha0tang.github.io/">Asst. Prof. Hao Tang</a> (PKU), <a href="https://bohanzhuang.github.io/">Prof. Bohan Zhuang</a> (ZJU), <a href="https://yangyangkiki.github.io/index.html">Dr. Yang Zhao</a> (La Trobe), <a href="https://scholar.google.com/citations?user=NIc4qPsAAAAJ&hl=en">Dr. Minh-Son To</a> (FHMRI), and many others. Zeyu is actively seeking opportunities and collaborations in both academia and industry.
              </p>                 
              <p style="text-align:center">
                <!-- <a href=""><img src="image/homepage.svg" style="height: 20px;vertical-align:middle;"></a> &nbsp; -->
                <a href="https://github.com/steve-zeyu-zhang"><img src="image/github.svg" style="height: 20px;vertical-align:middle;"></a>
                <a href="https://www.linkedin.com/in/steve-zeyu-zhang/"><img src="image/linkedin.svg" style="height: 30px;vertical-align:middle;"></a>
                <!-- <a href=""><img src="image/googlescholar.svg" style="height: 24px;vertical-align:middle;"></a> -->
                <a href="https://www.youtube.com/@SteveZeyuZhang"><img src="image/youtube.svg" style="height: 25px;vertical-align:middle;"></a>
                <!-- <a href=""><img src="image/instagram.svg" style="height: 26px;vertical-align:middle;"></a> -->
                &nbsp <a href="mailto:steve.zeyu.zhang@outlook.com"><img src="image/email.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                <a href="pdf/zeyu_zhang_resume.pdf"><img src="image/resume.svg" style="height: 22px;vertical-align:middle;"></a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:85%;max-width:85%" alt="profile photo" src="image/zeyu.jpeg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:5px 20px;width:100%;vertical-align:middle;">
              <h2 style="margin: 0;">News</h2>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:10px;vertical-align:middle">
              <b>(07/19/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/MotionAvatar/"><b>Motion Avatar</b></a> has been accepted to <a href="https://bmvc2024.org/"><b>BMVC 2024</b></a>!<br>
              <b>(07/02/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><b>Motion Mamba</b></a> has been accepted to <a href="https://eccv2024.ecva.net/"><b style="color: OrangeRed;">ECCV 2024</b></a>!<br>
              <b>(06/18/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/JointViT/"><b>JointViT</b></a> has been selected as <b style="color: red;">oral presentation</b> at <a href="https://miua2024.github.io/"><b>MIUA 2024</b></a>!<br>
              <b>(05/14/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/JointViT/"><b>JointViT</b></a> has been accepted to <a href="https://miua2024.github.io/"><b>MIUA 2024</b></a>!<br>
              <b>(03/13/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><b>Motion Mamba</b></a> has been featured in <a href="https://twitter.com/_akhaliq/status/1767750847239262532"><b>Daily Papers</b></a>!<br>
              <b>(02/10/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/SegReg/"><b>SegReg</b></a> has been accepted to <a href="https://biomedicalimaging.org/2024/"><b>ISBI 2024</b></a>!
            </td>
          </tr>
        </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Publications</h2>
              <p>
                Selected publications are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  

        <tr onmouseout="ld_stop()" onmouseover="ld_start()" bgcolor="#ffffd0">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/motionmamba.gif" width="320" height="180" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Motion Mamba: Efficient and Long Sequence Motion Generation</papertitle>
              <br>
              <strong>Zeyu Zhang</strong><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/akideliu/">Akide Liu</a><sup>∗</sup>,
              <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Ian Reid</a>,
              <a href="http://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a>,
              <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a>,
              <a href="https://ha0tang.github.io/">Hao Tang</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2403.07487"><img src="https://img.shields.io/badge/arXiv-2403.07487-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/MotionMamba"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/motion-mamba-efficient-and-long-sequence"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://huggingface.co/papers/2403.07487"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a>
              <a href="https://steve-zeyu-zhang.github.io/MotionMamba/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://eccv2024.ecva.net/"><strong><em style="color: OrangeRed;">ECCV 2024</em></strong><br></a>
              Human motion generation is a key goal in generative computer vision, and we propose Motion Mamba, a model using state space models (SSMs) with Hierarchical Temporal Mamba (HTM) and Bidirectional Spatial Mamba (BSM) blocks, achieving up to 50% FID improvement and 4x speedup on HumanML3D and KIT-ML datasets, showcasing efficient and high-quality long sequence motion modeling.
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/motionavatar.gif" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</papertitle>
              <br>
              <strong>Zeyu Zhang</strong><sup>∗</sup>,
              <a href="https://github.com/u7079256">Yiran Wang</a><sup>∗</sup>,
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/shuo-chen-7747a7246/">Shuo Chen</a>,
              <a href="https://github.com/Tiooo111">Zhiyuan Zhang</a>,
              <a href="https://github.com/gekelly">Shiya Huang</a>,
              <a href="https://zwbx.github.io/">Wenbo Zhang</a>,
              <a href="https://mengf1.github.io/">Meng Fang</a>,
              <a href="https://profiles.uts.edu.au/Ling.Chen">Ling Chen</a>,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/MotionAvatar/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2405.11286"><img src="https://img.shields.io/badge/arXiv-2405.11286-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/MotionAvatar"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/motion-avatar-generate-human-and-animal"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/MotionAvatar/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://bmvc2024.org/"><strong><em>BMVC 2024</em></strong><br></a>
              Our paper introduces a novel agent-based approach called Motion Avatar for generating customizable human and animal 3D avatars with motions via text queries, coordinated by an LLM planner, and supported by the new Zoo-300K animal motion dataset.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/jointvit.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>JointViT: Modeling Oxygen Saturation Levels with Joint Supervision on Long-Tailed OCTA</papertitle>
              <br>
              <strong>Zeyu Zhang</strong>,
              <a href="https://www.linkedin.com/in/xuyin-q-29672524a/">Xuyin Qi</a>,
              <a href="https://www.linkedin.com/in/mingxi-chen-4b57562a1/">Mingxi Chen</a>,
              <a href="https://github.com/lgX1123">Guangxi Li</a>,
              <a href="https://www.flinders.edu.au/people/ryan.pham">Ryan Pham</a>,
              <a href="https://www.flinders.edu.au/people/ayub.qassim">Ayub Qassim</a>,
              <a href="https://www.linkedin.com/in/ella-berry-a2a3aab4/">Ella Berry</a>,
              <a href="https://researchers.adelaide.edu.au/profile/zhibin.liao">Zhibin Liao</a>,
              <a href="https://researchnow.flinders.edu.au/en/persons/owen-siggs-2">Owen Siggs</a>,
              <a href="https://researchers.adelaide.edu.au/profile/robert.mclaughlin">Robert Mclaughlin</a>,
              <a href="https://www.flinders.edu.au/people/jamie.craig">Jamie Craig</a>,
              <a href="https://www.flinders.edu.au/people/minhson.to">Minh-Son To</a>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/JointViT/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2404.11525"><img src="https://img.shields.io/badge/arXiv-2404.11525-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/JointViT"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/jointvit-modeling-oxygen-saturation-levels"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/JointViT/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://miua2024.github.io/"><strong><em>MIUA 2024 Oral</em></strong><br></a>
              Our paper introduces JointViT, a Vision Transformer model with a novel joint loss function and balancing augmentation technique that significantly improves the accuracy of diagnosing sleep-related disorders using OCTA, achieving up to a 12.28% accuracy improvement.
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/segreg.gif" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>SegReg: Segmenting OARs by Registering MR Images and CT Annotations</papertitle>
              <br>
              <strong>Zeyu Zhang</strong>,
              <a href="https://www.linkedin.com/in/xuyin-q-29672524a/">Xuyin Qi</a>,
              <a href="https://www.adelaide.edu.au/directory/b.zhang">Bowen Zhang</a>,
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a>,
              <a href="https://iconcancercentre.com.au/doctor/hien-le">Hien Le</a>,
              <a href="https://www.linkedin.com/in/bora-jeong-5a3177231/">Bora Jeong</a>,
              <a href="https://researchers.adelaide.edu.au/profile/zhibin.liao">Zhibin Liao</a>,
              <a href="https://www.adelaide.edu.au/directory/yunxiang.liu">Yunxiang Liu</a>,
              <a href="https://researchers.adelaide.edu.au/profile/johan.verjans">Johan Verjans</a>,
              <a href="https://www.flinders.edu.au/people/minhson.to">Minh-Son To</a>,
              <a href="http://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/SegReg"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2311.06956"><img src="https://img.shields.io/badge/arXiv-2311.06956-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://openreview.net/forum?id=rC8bmJoOOTC"><img src="https://img.shields.io/badge/OpenReview-8c1b13?style=flat-square"></a>
              <a href="https://github.com/steve-zeyu-zhang/SegReg"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/segreg-segmenting-oars-by-registering-mr"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/SegReg/webpage/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://biomedicalimaging.org/2024/"><strong><em>ISBI 2024</em></strong><br></a>
              To improve the accuracy and efficiency of organ at risk (OAR) segmentation in radiotherapy, we propose SegReg, a method that combines CT and MRI using Elastic Symmetric Normalization, outperforming traditional CT-only methods by 16.78% in mDSC and 18.77% in mIoU.
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()"">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/tta.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Thin-Thick Adapter: Segmenting Thin Scans Using Thick Annotations</papertitle>
              <br>
              <strong>Zeyu Zhang</strong>,
              <a href="https://www.adelaide.edu.au/directory/b.zhang">Bowen Zhang</a>,
              <a href="https://www.nrf.com.au/news/rotem-latest-research">Abhiram Hiwase</a>,
              <a href="https://www.researchgate.net/scientific-contributions/Feng-Chen-2262634351">Feng Chen</a>,
              <a href="https://www.linkedin.com/in/akideliu/">Akide Liu</a>,
              <a href="https://jonesradiology.com.au/barras-christen">Christen Barras</a>,
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a>,
              <a href="https://www.dradamwells.com.au/">Adam Wells</a>,
              <a href="https://www.thefrontiersconference.org/daniel-ellis">Daniel Ellis</a>,
              <a href="https://www.adelaide.edu.au/directory/benjamin.reddi">Benjamin Reddi</a>,
              <a href="https://www.linkedin.com/in/andrew-burgan-572334108/?originalSubdomain=au">Andrew Burgan</a>,
              <a href="https://www.flinders.edu.au/people/minhson.to">Minh-Son To</a>,
              <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Ian Reid</a>,
              <a href="http://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a>,
              <a href="https://scholar.google.com/citations?user=ddDL9HMAAAAJ&hl=zh-CN">Yutong Xie</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://openreview.net/forum?id=NF5uhYkI9C"><img src="https://img.shields.io/badge/OpenReview-8c1b13?style=flat-square"></a>
              <a href="https://steve-zeyu-zhang.github.io/ThinThickAdapter/website/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Medical imaging segmentation is critical for medical analysis, predominantly using thicker CT slices due to the scarcity of annotated thin slices, so we propose segmenting thin scans with thicker slice annotations, introduce the CQ500-Thin dataset, and present the Thin-Thick Adapter to bridge domain gaps, significantly improving segmentation performance.
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/segstitch.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>SegStitch: Multidimensional Transformer for Robust and Efficient Medical Imaging Segmentation</papertitle>
              <br>
              <a href="https://github.com/goblin327">Shengbo Tan</a>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://ieeexplore.ieee.org/author/37087137422">Ying Cai</a><sup>✉</sup>,
              <a href="https://ieeexplore.ieee.org/author/37085795653">Daji Ergu</a>,
              <a href="https://dblp.org/pid/65/6292.html">Lin Wu</a>,
              <a href="https://dl.acm.org/profile/99661184796">Binbin Hu</a>,
              <a href="https://www.researchgate.net/search/researcher?q=Pengzhang+Yu">Pengzhang Yu</a>,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2408.00496"><img src="https://img.shields.io/badge/arXiv-2408.00496-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/goblin327/SegStitch"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href=""><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/goblin327/SegStitch/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Medical imaging segmentation, crucial for lesion analysis, has seen advances with transformers in 3D segmentation. Despite their scalability, transformers struggle with local features and complexity. We propose SegStitch, combining transformers with denoising ODE blocks, improving mDSC by up to 11.48% and reducing parameters by 36.7%, promising real-world clinical adaptation.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/lowrank.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Sine Activated Low-Rank Matrices for Parameter Efficient Learning</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/yiping-ji-111b97142/">Yiping Ji</a><sup>∗</sup>,
              <a href="https://researchers.adelaide.edu.au/profile/hemanth.saratchandran">Hemanth Saratchandran</a><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/camerondgordon">Cameron Gordon</a>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://researchers.adelaide.edu.au/profile/simon.lucey">Simon Lucey</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2403.19243"><img src="https://img.shields.io/badge/arXiv-2403.19243-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://paperswithcode.com/paper/sine-activated-low-rank-matrices-for"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              We propose a novel theoretical framework integrating a sinusoidal function into low-rank decomposition, enhancing parameter efficiency and model accuracy across diverse neural network applications such as Vision Transformers, Large Language Models, Neural Radiance Fields, and 3D shape modeling.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/xlip.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a><sup>∗</sup>,
              <a href="https://v3alab.github.io/author/yutong-xie/">Yutong Xie</a><sup>∗</sup>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=gSEw8EsAAAAJ&hl=en">Minh Hieu Phan</a>,
              <a href="https://scholar.google.com/citations?user=OgKU77kAAAAJ&hl=zh-CN">Qi Chen</a>,
              <a href="https://profiles.uts.edu.au/Ling.Chen">Ling Chen</a>,
              <a href="http://www.qi-wu.me/">Qi Wu</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2407.19546"><img src="https://img.shields.io/badge/arXiv-2407.19546-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/White65534/XLIP"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/xlip-cross-modal-attention-masked-modelling"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/White65534/XLIP/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Vision-and-language pretraining (VLP) in the medical field faces challenges with reconstructing pathological features due to data scarcity and limited use of paired/unpaired data. This paper proposes XLIP, using AttMIM and EntMLM modules, to enhance feature learning from unpaired data, achieving state-of-the-art results in medical classification tasks.
            </td>
          </tr>



          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/bhsd.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>BHSD: A 3D Multi-class Brain Hemorrhage Segmentation Dataset</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a>,
              <a href="https://v3alab.github.io/author/yutong-xie/">Yutong Xie</a>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://github.com/jinchaogjc">Jinchao Ge</a>,
              <a href="https://radiopaedia.org/users/kaspar-lewis-yaxley?lang=us">Kaspar Yaxley</a>,
              <a href="https://au.linkedin.com/in/suzan-bahadir-57870416b">Suzan Bahadir</a>,
              <a href="http://www.qi-wu.me/">Qi Wu</a>,
              <a href="https://scholar.google.com/citations?user=ksQ4JnQAAAAJ&hl=zh-CN">Yifan Liu</a>,
              <a href="https://www.flinders.edu.au/people/minhson.to">Minh-Son To</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://doi.org/10.1007/978-3-031-45673-2_15"><img src="https://img.shields.io/badge/DOI-10.1007%2F978--3--031--45673--2__15-fcb520?style=flat-square&logo=doi"></a>
              <a href="https://arxiv.org/abs/2311.06956"><img src="https://img.shields.io/badge/arXiv-2311.06956-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://openreview.net/forum?id=R5951Pk_vlw"><img src="https://img.shields.io/badge/OpenReview-8c1b13?style=flat-square"></a>
              <a href="https://github.com/White65534/BHSD"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/bhsd-a-3d-multi-class-brain-hemorrhage"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://paperswithcode.com/dataset/bhsd"><img src="https://img.shields.io/badge/Papers%20With%20Code-Dataset-21cbce.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/White65534/BHSD/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://sites.google.com/view/mlmi2023"><strong><em>MLMI 2023</em></strong><br></a>
              The Brain Hemorrhage Segmentation Dataset (BHSD) is a comprehensive 3D multi-class ICH dataset with pixel-level and slice-level annotations designed to support supervised and semi-supervised ICH segmentation tasks, addressing the lack of existing public datasets for multi-class ICH segmentation.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/diabetes.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>A Deep Learning Approach to Diabetes Diagnosis</papertitle>
              <br>
              <strong>Zeyu Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=rUJ9DVAAAAAJ&hl=en">Khandaker Asif Ahmed</a>,
              <a href="https://staffportal.curtin.edu.au/staff/profile/view/rakibul-hasan-145a1046/">Md Rakibul Hasan</a>,
              <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Tom Gedeon</a>,
              <a href="https://radiopaedia.org/users/kaspar-lewis-yaxley?lang=us">Md Zakir Hossain</a>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/DiabetesDiagnosis"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2403.07483"><img src="https://img.shields.io/badge/arXiv-2403.07483-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/DiabetesDiagnosis"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/a-deep-learning-approach-to-diabetes"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://aciids.pwr.edu.pl/2024/"><strong><em>ACIIDS 2024</em></strong><br></a>
              We propose a non-invasive diabetes diagnosis method using a Back Propagation Neural Network with batch normalization, addressing class imbalance and improving performance over traditional methods, achieving high accuracy on multiple datasets.
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Research Experience</h2>
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/pku.png" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Visiting Student Researcher</papertitle>
              <br>
              <a href="https://english.pku.edu.cn/">Peking University</a>
              <br>
              <em>July 2024 - Present</em>
              <br>
              Worked on 3D human motion generation, advised by <a href="https://ha0tang.github.io/">Asst. Prof. Hao Tang</a> (PKU).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/mbzuai.jpg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Intern</papertitle>
              <br>
              <a href="https://mbzuai.ac.ae/">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</a>
              <br>
              <em>May 2024 - June 2024</em>
              <br>
              Worked on unsupervised classification of cellular structures based on cryo-electron tomography (cryo-ET), hosted by <a href="https://xulabs.github.io/min-xu/">Assoc. Prof. Min Xu</a> (CMU, MBZUAI) and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a> (MBZUAI, AIML).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/latrobe.jpeg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Assistant</papertitle>
              <br>
              <a href="https://www.latrobe.edu.au/">La Trobe University</a>
              <br>
              <em>Apr 2024 - Present</em>
              <br>
              Worked on 3D generation and AI for Heath, hosted by <a href="https://yangyangkiki.github.io/">Dr. Yang Zhao</a> (La Trobe University).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/monash.jpeg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Assistant</papertitle>
              <br>
              <a href="https://www.monash.edu/">Monash University</a>
              <br>
              <em>Feb 2024 - May 2024</em>
              <br>
              Worked on 3D/4D generative learning, specifically focusing on text-guided human motion and avatar generation, hosted by <a href="https://users.monash.edu.au/~gholamrh/">Assoc. Prof. Reza Haffari</a> (Monash University), and also worked with <a href="https://bohanzhuang.github.io/">Prof. Bohan Zhuang</a> (ZJU, Monash University).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/aiml.jpg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Visiting Student Researcher</papertitle>
              <br>
              <a href="https://www.adelaide.edu.au/aiml/">Australian Institute for Machine Learning (AIML)</a>
              <br>
              <em>Nov 2022 - Jan 2024</em>
              <br>
              Worked on 3D medical imaging analysis, with a particular focus on semantic segmentations of tumors, hemorrhages, and organs at risk, advised by <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a> (MBZUAI, AIML), also worked with <a href="https://www.adelaide.edu.au/directory/b.zhang">Dr. Bowen Zhang</a> (AIML), <a href="https://researchers.adelaide.edu.au/profile/yutong.xie">Dr. Yutong Xie</a> (AIML), and <a href="https://scholar.google.com/citations?user=OgKU77kAAAAJ&hl=zh-CN">Dr. Qi Chen</a> (AIML).
              </td>
            </tr>


            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/fhmri.png" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Student Researcher</papertitle>
              <br>
              <a href="https://www.flinders.edu.au/health-medical-research-institute">Flinders Health and Medical Research Institute (FHMRI)</a>
              <br>
              <em>Nov 2022 - Present</em>
              <br>
              Worked on 3D medical imaging analysis, particularly in the realms of 2D and 3D medical representation learning and explainable AI, hosted by <a href="https://scholar.google.com/citations?user=NIc4qPsAAAAJ&hl=en">Dr. Minh-Son To</a> (FHMRI).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/nci.jpeg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Intern</papertitle>
              <br>
              <a href="https://nci.org.au/">National Computational Infrastructure (NCI)</a>
              <br>
              <em>Feb 2023 - Jun 2023</em>
              <br>
              Worked on long tail large scale multi-label text classification, hosted by <a href="https://nci.org.au/research/people/dr-jingbo-wang">Dr. Jingbo Wang</a> (NCI).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/anu.jpg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Student Researcher</papertitle>
              <br>
              <a href="https://www.anu.edu.au/">The Australian National University (ANU)</a>
              <br>
              <em>Jul 2022 - Nov 2022</em>
              <br>
              Worked on diabetes diagnosis in deep learning, advised by <a href="https://radiopaedia.org/users/kaspar-lewis-yaxley?lang=us">Dr. Md Zakir Hossain</a> (ANU, Curtin University, CSIRO Data61), also worked with <a href="https://scholar.google.com/citations?user=rUJ9DVAAAAAJ&hl=en">Dr. Khandaker Asif Ahmed</a> (CSIRO), <a href="https://staffportal.curtin.edu.au/staff/profile/view/rakibul-hasan-145a1046/"> Mr. Md Rakibul Hasan</a> (Curtin University, Brac University), and <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Prof. Tom Gedeon</a> (Curtin University, ANU, Óbuda University).
              </td>
            </tr>  

          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Education</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;vertical-align:middle">
              <img src="image/anu.jpg" width="50" style="border-style: none">
            </td>
            <td style="padding:10px;vertical-align:middle;width:85%;">
              <papertitle>Bachelor of Science (Advanced) (Honours)</papertitle>
            <br>
            <a href="https://www.anu.edu.au/">The Australian National University (ANU)</a>
            <br>
            <em>Jul 2021 - Jun 2025 (Expected)</em>
            <br>
            Major: Computer Science, Minor: Mathematics
            </td>
          </tr>  
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;vertical-align:middle">
              <img src="image/ic.jpg" width="50" style="border-style: none">
            </td>
            <td style="padding:10px;vertical-align:middle;width:85%;">
              <papertitle>Visiting Student</papertitle>
            <br>
            <a href="https://www.imperial.ac.uk/">Imperial College London</a>
            <br>
            <em>Jul 2022</em>
            <br>
            Quantitative Sciences Research Institute (QSRI)
            </td>
          </tr>  
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;vertical-align:middle">
              <img src="image/ucl.svg" width="50" style="border-style: none">
            </td>
            <td style="padding:10px;vertical-align:middle;width:85%;">
              <papertitle>Visiting Student</papertitle>
            <br>
            <a href="https://www.ucl.ac.uk/">University College London (UCL)</a>
            <br>
            <em>Jul 2022</em>
            <br>
            </td>
          </tr>  
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;vertical-align:middle">
              <img src="image/sjtu.jpeg" width="50" style="border-style: none">
            </td>
            <td style="padding:10px;vertical-align:middle;width:85%;">
              <papertitle>Visiting Student</papertitle>
            <br>
            <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>
            <br>
            <em>Dec 2021 - Jan 2022</em>
            <br>
            </td>
          </tr>  
        </tbody></table>


          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Academic Services</h2>
              </td>
            </tr>
          </tbody></table>


          <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	      
        </tbody></table> -->

        <p style="height: 40px;">&nbsp;</p><br>
      </td>
    </tr>
  </table>
</body>

</html>
<!--for page update--> 
