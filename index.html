<!DOCTYPE HTML>
<html lang="en">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Zeyu Zhang</title>
      <meta name="author" content="Zeyu Zhang">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel="stylesheet"
         href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@700&family=Noto+Sans:wght@400;500;600;700&display=swap">
      <link rel="stylesheet" type="text/css" href="stylesheet.css">
      <link rel="icon" type="image/png" href="image/zzhang.svg">
      <style>
         .spacer {
             height: 5px;
         }
     </style>
     <style>
       .spacer2 {
           height: 1px;
       }
     </style>
   </head>
   <body>
      <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
         <tbody>
            <tr style="padding:0px">
               <td style="padding:0px">
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr style="padding:0px">
                           <td style="padding:2.5%;width:30%;max-width:30%">
                              <a href="image/zeyu_round.png"><img style="width:100%;max-width:100%" alt="profile photo" src="image/zeyu_round.png" class="hoverZoomLink"></a>
                           </td>
                           <td style="padding:2.5%;width:70%;vertical-align:middle">
                              <p style="text-align:center">
                                 <name>Zeyu Zhang</name>
                              </p>
                              <p>
                                Zeyu Zhang is an undergraduate researcher.
                              </p>
                              <p>
                                His research interests lie in geometric generative modeling and its applications to multimodal foundation models, spatial intelligence, embodied AI, and AI for health.
                              </p>
                              <p>
                                He received his bachelor’s degree at <a href="https://www.anu.edu.au">the Australian National University</a>, advised by <a href="http://users.cecs.anu.edu.au/~hartley/">Prof. Richard Hartley</a> and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>.
                              </p>
                              <p>
                                 <b style="color: red;">Zeyu is actively seeking a PhD, research engineer, or research intern position for Fall 2026 in the US.</b>
                              </p>
                              <p style="text-align:center">
                                 <!-- <a href=""><img src="image/homepage.svg" style="height: 20px;vertical-align:middle;"></a> &nbsp; -->
                                 <a href="https://github.com/steve-zeyu-zhang"><img src="image/github.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                                 <a href="https://www.linkedin.com/in/steve-zeyu-zhang/"><img src="image/linkedin.svg" style="height: 30px;vertical-align:middle;"></a> &nbsp
                                 <a href="https://scholar.google.com/citations?user=CbsajL8AAAAJ"><img src="image/googlescholar_new.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                                 <a href="https://www.youtube.com/@SteveZeyuZhang"><img src="image/youtube.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                                 <a href="https://x.com/SteveZeyuZhang"><img src="image/x.svg" style="height: 23px;vertical-align:middle;"></a> &nbsp
                                 <a href="https://www.instagram.com/stevezeyuzhang"><img src="image/instagram.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                                 <a href="https://xhslink.com/m/9EdUdrX8JOs"><img src="image/xhs.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                                 <a href="https://ieeexplore.ieee.org/author/824177461533097"><img src="image/ieee.jpeg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                                 <a href="https://orcid.org/0009-0006-8819-3741"><img src="image/orcid.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                                 <a href="https://dblp.org/pid/44/8352-6.html"><img src="image/dblp.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                                 <a href="mailto:steve.zeyu.zhang@outlook.com"><img src="image/email.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                                 <a href="pdf/zeyu_zhang_resume.pdf"><img src="image/resume.svg" style="height: 22px;vertical-align:middle;"></a>
                               </p>
                           </td>
                        </tr>
                     </tbody>
                  </table>




                  <hr>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:100%;vertical-align:middle">
                              <br>
                              <heading>News</heading>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                           <td style="padding:10px;vertical-align:middle">
                            <b>(09/18/2025)</b> &#127881; Our paper <a href="#"><b>FlashMo</b></a> has been accepted to <a href="https://neurips.cc/Conferences/2025"><b style="color: OrangeRed;">NeurIPS 2025</b></a>!<br>
                            <b>(08/05/2025)</b> &#127881; Our paper <a href="https://aigeeksgroup.github.io/3D-R1"><b>3D-R1</b></a> has been shared in <a href="https://x.com/_akhaliq/status/1952533689583181932"><b>Daily Papers</b></a> by AK!<br>
                             <b>(07/02/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><b>Motion Mamba</b></a> has been accepted to <a href="https://eccv2024.ecva.net/"><b style="color: OrangeRed;">ECCV 2024</b></a>!<br>
                             <b>(03/13/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><b>Motion Mamba</b></a> has been shared in <a href="https://x.com/_akhaliq/status/1767750847239262532"><b>Daily Papers</b></a> by AK!<br>
                           </td>
                        </tr>
                     </tbody>
                  </table>



                  
                  <hr>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:100%;vertical-align:middle">
                              <br>
                              <heading>Publications</heading>
                              <p>
                                 Selected publications are <span class="highlight">highlighted</span>. (<sup>∗</sup>Equal contribution. <sup>✝</sup>Project lead. <sup>✉</sup>Corresponding author.)
                               </p>
                               <!-- Toggle Button -->
                               <button id="toggleButton" class="toggle-btn" onclick="togglePublications()">Show all</button>
                           </td>
                        </tr>
                     </tbody>
                  </table>



                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


                    <tr onmouseout="ld_stop()" onmouseover="ld_start()" bgcolor="#ffffd0">
                      <td style="padding:0px;width:25%;vertical-align:middle">
                        <img src="image/flashmo.svg" width="320" height="180" style="border-style: none">
                      </td>
                      <td style="padding:10px;width:75%;vertical-align:middle">
                          <papertitle>FlashMo: Geometric Interpolants and Frequency-Aware Sparsity for Scalable Efficient Motion Generation</papertitle>
                        <br>
                        <strong>Zeyu Zhang</strong><sup>∗</sup>,
                        Yiran Wang<sup>∗</sup>,
                        Danning Li<sup>∗</sup>,
                        Dong Gong,
                        Ian Reid,
                        Richard Hartley
                        <!-- <br><div class="spacer"></div> -->
                        <!-- <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a> -->
                        <!-- <a href="https://arxiv.org/abs/2403.07487"><img src="https://img.shields.io/badge/arXiv-2403.07487-b31b1b?style=flat-square&logo=arxiv"></a> -->
                        <!-- <a href="https://github.com/steve-zeyu-zhang/MotionMamba"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a> -->
                        <!-- <a href="https://paperswithcode.com/paper/motion-mamba-efficient-and-long-sequence"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                        <!-- <a href="https://huggingface.co/papers/2403.07487"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a> -->
                        <!-- <a href="https://steve-zeyu-zhang.github.io/MotionMamba/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
                        <br>
                        <a href="https://neurips.cc/Conferences/2025"><strong><em style="color: OrangeRed;">NeurIPS 2025</em></strong></a><br>
                        FlashMo introduces a geometric factorized interpolant and frequency-sparse attention, enabling scalable efficient 3D motion diffusion. Experiments show superior quality, efficiency, and scalability over state-of-the-art baselines.
                      </td>
                    </tr>


                    <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                      <td style="padding:0px;width:25%;vertical-align:middle">
                        <!-- <img src="image/flashmo.svg" width="320" height="180" style="border-style: none"> -->
                        <video src="image/fpsattention.mp4" width="320" autoplay muted loop playsinline style="border-style: none;"></video>
                      </td>
                      <td style="padding:10px;width:75%;vertical-align:middle">
                          <papertitle>FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion</papertitle>
                        <br>
                        Akide Liu<sup>∗</sup>,
                        <strong>Zeyu Zhang</strong><sup>∗</sup>,
                        Zhexin Li<sup></sup>,
                        Xuehai Bai<sup></sup>,
                        Yizeng Han<sup></sup>,
                        Jiasheng Tang<sup></sup>,
                        Yuanjie Xing<sup></sup>,
                        Jichao Wu<sup></sup>,
                        Mingyang Yang<sup></sup>,
                        Weihua Chen<sup></sup>,
                        Jiahao He<sup></sup>,
                        Yuanyu He<sup></sup>,
                        Fan Wang<sup></sup>,
                        Gholamreza Haffari<sup></sup>,
                        Bohan Zhuang<sup></sup>
                        <br><div class="spacer"></div>
                        <a href="https://fps.ziplab.co/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
                        <a href="https://arxiv.org/abs/2506.04648"><img src="https://img.shields.io/badge/arXiv-2506.04648-b31b1b?style=flat-square&logo=arxiv"></a>
                        <!-- <a href="https://github.com/steve-zeyu-zhang/MotionMamba"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a> -->
                        <!-- <a href="https://paperswithcode.com/paper/motion-mamba-efficient-and-long-sequence"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                        <!-- <a href="https://huggingface.co/papers/2403.07487"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a> -->
                        <!-- <a href="https://steve-zeyu-zhang.github.io/MotionMamba/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
                        <br>
                        <a href="https://neurips.cc/Conferences/2025"><strong><em style="color: OrangeRed;">NeurIPS 2025</em></strong> <strong style="color: red;"><em>Spotlight</em></strong></a><br>
                        FPSAttention is a training-aware FP8 quantization and sparsity co-design for video diffusion models that achieves up to 7.09x kernel speedups and 4.96× E2E speedups without quality loss by aligning 3D tile granularity, denoising-step adaptation, and hardware-efficient kernels.
                      </td>
                    </tr>


                    <tr class="extra-pubs" style="display:none;" onmouseout="ld_stop()" onmouseover="ld_start()">
                      <td style="padding:0px;width:25%;vertical-align:middle">
                        <!-- <img src="image/flashmo.svg" width="320" height="180" style="border-style: none"> -->
                        <img src="image/zpressor.jpg" width="320" height="180" style="border-style: none">
                      </td>
                      <td style="padding:10px;width:75%;vertical-align:middle">
                          <papertitle>ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS</papertitle>
                        <br>
                        Weijie Wang<sup></sup>,
                        Yuedong Chen<sup></sup>,
                        <strong>Zeyu Zhang</strong><sup></sup>,
                        Duochao Shi<sup></sup>,
                        Akide Liu<sup></sup>,
                        Bohan Zhuang<sup></sup>
                        <br><div class="spacer"></div>
                        <a href="https://lhmd.top/zpressor/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
                        <a href="https://arxiv.org/abs/2505.23734"><img src="https://img.shields.io/badge/arXiv-2505.23734-b31b1b?style=flat-square&logo=arxiv"></a>
                        <a href="https://github.com/ziplab/ZPressor"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
                        <!-- <a href="https://paperswithcode.com/paper/motion-mamba-efficient-and-long-sequence"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                        <!-- <a href="https://huggingface.co/papers/2403.07487"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a> -->
                        <!-- <a href="https://steve-zeyu-zhang.github.io/MotionMamba/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
                        <br>
                        <a href="https://neurips.cc/Conferences/2025"><strong><em style="color: OrangeRed;">NeurIPS 2025</em></strong></a><br>
                        ZPressor is an architecture-agnostic module that compresses multi-view inputs for scalable feed-forward 3DGS.
                      </td>
                    </tr>


                    <tr class="extra-pubs" style="display:none;" onmouseout="ld_stop()" onmouseover="ld_start()">
                      <td style="padding:0px;width:25%;vertical-align:middle">
                        <!-- <img src="image/flashmo.svg" width="320" height="180" style="border-style: none"> -->
                        <img src="image/trico.png" width="320" style="border-style: none">
                      </td>
                      <td style="padding:10px;width:75%;vertical-align:middle">
                          <papertitle>TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning</papertitle>
                        <br>
                        Hongyang He<sup></sup>,
                        Xinyuan Song<sup></sup>,
                        Yangfan He<sup></sup>,
                        <strong>Zeyu Zhang</strong><sup></sup>,
                        Yanshu Li<sup></sup>,
                        Haochen You<sup></sup>,
                        Lifan Sun<sup></sup>,
                        Wenqiao Zhang<sup></sup>
                        <!-- <br><div class="spacer"></div> -->
                        <!-- <a href="https://fps.ziplab.co/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a> -->
                        <!-- <a href="https://arxiv.org/abs/2506.04648"><img src="https://img.shields.io/badge/arXiv-2403.07487-b31b1b?style=flat-square&logo=arxiv"></a> -->
                        <!-- <a href="https://github.com/steve-zeyu-zhang/MotionMamba"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a> -->
                        <!-- <a href="https://paperswithcode.com/paper/motion-mamba-efficient-and-long-sequence"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                        <!-- <a href="https://huggingface.co/papers/2403.07487"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a> -->
                        <!-- <a href="https://steve-zeyu-zhang.github.io/MotionMamba/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
                        <br>
                        <a href="https://neurips.cc/Conferences/2025"><strong><em style="color: OrangeRed;">NeurIPS 2025</em></strong></a><br>
                        TRiCo introduces a triadic game-theoretic co-training framework with two students, a meta-learned teacher, and an adversarial generator, leveraging mutual information pseudo-labeling to achieve state-of-the-art semi-supervised learning performance.
                      </td>
                    </tr>




                    <tr class="extra-pubs" style="display:none;" onmouseout="ld_stop()" onmouseover="ld_start()">
                      <td style="padding:0px;width:25%;vertical-align:middle">
                        <img src="image/ocrt.png" width="320" style="border-style: none">
                      </td>
                      <td style="padding:10px;width:75%;vertical-align:middle">
                          <papertitle>OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad</papertitle>
                        <br>
                        Luyao Tang,
                        Chaoqi Chen,
                        Yuxuan Yuan,
                        <strong>Zeyu Zhang</strong>,
                        Yue Huang,
                        Kun Zhang
                        <br><div class="spacer"></div>
                        <a href="https://arxiv.org/abs/2503.18695"><img src="https://img.shields.io/badge/arXiv-2503.18695-b31b1b?style=flat-square&logo=arxiv"></a>
                        <a href="https://github.com/lytang63/OCRT"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
                        <!-- <a href="https://paperswithcode.com/paper/ocrt-boosting-foundation-models-in-the-open"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                        <!-- <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
                        <br>
                        <a href="https://cvpr.thecvf.com/Conferences/2025"><strong><em style="color: OrangeRed;">CVPR 2025</em></strong></a><br>
                        Foundation models struggle with distribution shifts and weak supervision. We propose OCRT, a framework extracting high-level concepts and relations, enhancing SAM and CLIP generalizability in diverse tasks.
                      </td>
                    </tr>
                         
                    <tr class="extra-pubs" style="display:none;" onmouseout="ld_stop()" onmouseover="ld_start()">
                      <td style="padding:0px;width:25%;vertical-align:middle">
                        <img src="image/hazard.png" width="320" style="border-style: none">
                      </td>
                      <td style="padding:10px;width:75%;vertical-align:middle">
                          <papertitle>Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies</papertitle>
                        <br>
                        Zirui Song,
                        Guangxian Ouyang,
                        Meng Fang,
                        Hongbin Na,
                        Zijing Shi,
                        Zhenhao Chen,
                        Yujie Fu,
                        <strong>Zeyu Zhang</strong>,
                        Shiyu Jiang,
                        Miao Fang,
                        Ling Chen,
                        Xiuying Chen<sup>✉</sup>
                        <br><div class="spacer"></div>
                        <a href="https://arxiv.org/abs/2411.00781"><img src="https://img.shields.io/badge/arXiv-2411.00781-b31b1b?style=flat-square&logo=arxiv"></a>
                        <br>
                        <a href="https://2025.naacl.org/"><strong><em>NAACL 2025</em></strong></a><br>
                        Household robots struggle to detect hazards. We propose anomaly scenario generation using multi-agent brainstorming and 3D simulations, enhancing robotic skills in hazard detection, hygiene management, and child safety through diverse environments.
                      </td>
                    </tr>
          
          
                    <tr class="extra-pubs" style="display:none;" onmouseout="ld_stop()" onmouseover="ld_start()">
                      <td style="padding:0px;width:25%;vertical-align:middle">
                        <img src="image/lowrank.svg" width="320" style="border-style: none">
                      </td>
                      <td style="padding:10px;width:75%;vertical-align:middle">
                          <papertitle>Efficient Learning With Sine-Activated Low-rank Matrices</papertitle>
                        <br>
                        Yiping Ji<sup></sup>,
                        Hemanth Saratchandran,
                        Cameron Gordon,
                        <strong>Zeyu Zhang</strong>,
                        Simon Lucey
                        <br><div class="spacer"></div>
                        <a href="https://yipingji.github.io/sine_activated_PEL"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
                        <a href="https://arxiv.org/abs/2403.19243"><img src="https://img.shields.io/badge/arXiv-2403.19243-b31b1b?style=flat-square&logo=arxiv"></a>
                        <a href="https://openreview.net/forum?id=cWGCkd7mCp"><img src="https://img.shields.io/badge/OpenReview-8c1b13?style=flat-square"></a>
                        <a href="https://github.com/yipingji/Sine-Low-Rank"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
                        <!-- <a href="https://paperswithcode.com/paper/sine-activated-low-rank-matrices-for"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                        <a href="https://steve-zeyu-zhang.github.io/scholar/sine.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
                        <br>
                        <a href="https://iclr.cc/Conferences/2025"><strong><em style="color: OrangeRed;">ICLR 2025</em></strong></a><br>
                        We propose a novel theoretical framework integrating a sinusoidal function into low-rank decomposition, enhancing parameter efficiency and model accuracy across diverse neural network applications such as Vision Transformers, Large Language Models, Neural Radiance Fields, and 3D shape modeling.
                      </td>
                    </tr>

                    <tr onmouseout="ld_stop()" onmouseover="ld_start()" bgcolor="#ffffd0">
                     <td style="padding:0px;width:25%;vertical-align:middle">
                      <video src="image/motionmamba.mp4" width="320" autoplay muted loop playsinline style="border-style: none;"></video>
                     </td>
                     <td style="padding:10px;width:75%;vertical-align:middle">
                         <papertitle>Motion Mamba: Efficient and Long Sequence Motion Generation</papertitle>
                       <br>
                       <strong>Zeyu Zhang</strong><sup>∗</sup>,
                       Akide Liu<sup>∗</sup>,
                       Ian Reid,
                       Richard Hartley,
                       Bohan Zhuang,
                       Hao Tang
                       <br><div class="spacer"></div>
                       <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
                       <a href="https://arxiv.org/abs/2403.07487"><img src="https://img.shields.io/badge/arXiv-2403.07487-b31b1b?style=flat-square&logo=arxiv"></a>
                       <a href="https://github.com/steve-zeyu-zhang/MotionMamba"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
                       <!-- <a href="https://paperswithcode.com/paper/motion-mamba-efficient-and-long-sequence"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                       <a href="https://huggingface.co/papers/2403.07487"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a>
                       <a href="https://steve-zeyu-zhang.github.io/MotionMamba/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
                       <br>
                       <a href="https://eccv2024.ecva.net/"><strong><em style="color: OrangeRed;">ECCV 2024</em></strong></a><br>
                       Human motion generation is a key goal in generative computer vision, and we propose Motion Mamba, a model using state space models (SSMs) with Hierarchical Temporal Mamba (HTM) and Bidirectional Spatial Mamba (BSM) blocks, achieving up to 50% FID improvement and 4x speedup on HumanML3D and KIT-ML datasets, showcasing efficient and high-quality long sequence motion modeling.
                     </td>
                   </tr>


           
   
           
                   </tbody></table>
          


                    <hr>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr>
                              <td style="width:100%;vertical-align:middle">
                                <br>
                                <heading>Research Projects</heading>
                                <!-- <p>
                                    Selected publications are <span class="highlight">highlighted</span>. (<sup>∗</sup>Equal contribution. <sup>✝</sup>Project lead. <sup>✉</sup>Corresponding author.)
                                  </p> -->
                              </td>
                          </tr>
                        </tbody>
                    </table>

                    <br>
           
                     <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


                      <tr style="display:none;" onmouseout="ld_stop()" onmouseover="ld_start()">
                        <td style="padding:0px;width:25%;vertical-align:middle">
                          <video src="image/blockvid.mp4" width="320" autoplay muted loop playsinline style="border-style: none;"></video>
                        </td>
                        <td style="padding:10px;width:75%;vertical-align:middle">
                            <papertitle>BlockVid: Block Diffusion for High-Fidelity and Coherent Minute-Long Video Generation</papertitle>
                          <br>
                          <strong>Zeyu Zhang</strong>,
                          Shuning Chang,
                          Yuanyu He,
                          Yizheng Han,
                          Jiasheng Tang,
                          Fan Wang,
                          Bohan Zhuang<sup>✉</sup>
                          <br><div class="spacer"></div>
                          <a href="https://ziplab.co/BlockVid/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
                          <!-- <a href="https://arxiv.org/abs/2509.10884"><img src="https://img.shields.io/badge/arXiv-2509.10884-b31b1b?style=flat-square&logo=arxiv"></a> -->
                          <!-- <a href="https://github.com/GigaAI-research/VLA-R1"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a> -->
                          <!-- <a href="https://paperswithcode.com/paper/motion-anything-any-to-motion-generation"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                          <!-- <a href="https://huggingface.co/papers/2509.10884"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a> -->
                          <!-- <a href="https://github.com/AIGeeksGroup/Nav-R1#%EF%B8%8F-citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
                          <br>
                          <!-- <strong><em>Preprint</em></strong><br> -->
                          BlockVid is a semi-AR block diffusion framework equipped with semantic sparse KV caching, block forcing, and noise scheduling. Furthermore, LV-Bench is a fine-grained benchmark for minute-long videos with dedicated metrics to evaluate long-range coherence. 
                        </td>
                      </tr>


                      <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                        <td style="padding:0px;width:25%;vertical-align:middle">
                          <video src="image/vlar1.mp4" width="320" autoplay muted loop playsinline style="border-style: none;"></video>
                        </td>
                        <td style="padding:10px;width:75%;vertical-align:middle">
                            <papertitle>VLA-R1: Enhancing Reasoning in Vision-Language-Action Models</papertitle>
                          <br>
                          Angen Ye<sup>∗</sup>,
                          <strong>Zeyu Zhang</strong><sup>∗</sup>,
                          Boyuan Wang,
                          Xiaofeng Wang,
                          Dapeng Zhang,
                          Zheng Zhu<sup>✉</sup>
                          <br><div class="spacer"></div>
                          <a href="https://gigaai-research.github.io/VLA-R1/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
                          <!-- <a href="https://arxiv.org/abs/2509.10884"><img src="https://img.shields.io/badge/arXiv-2509.10884-b31b1b?style=flat-square&logo=arxiv"></a> -->
                          <a href="https://github.com/GigaAI-research/VLA-R1"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
                          <!-- <a href="https://paperswithcode.com/paper/motion-anything-any-to-motion-generation"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                          <!-- <a href="https://huggingface.co/papers/2509.10884"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a> -->
                          <!-- <a href="https://github.com/AIGeeksGroup/Nav-R1#%EF%B8%8F-citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
                          <br>
                          <!-- <strong><em>Preprint</em></strong><br> -->
                          VLA-R1 is a reasoning-enhanced vision–language–action model that enables step-by-step reasoning and robust action execution across diverse tasks and domains.
                        </td>
                      </tr>

                      <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                        <td style="padding:0px;width:25%;vertical-align:middle">
                          <video src="image/navr1.mp4" width="320" autoplay muted loop playsinline style="border-style: none;"></video>
                        </td>
                        <td style="padding:10px;width:75%;vertical-align:middle">
                            <papertitle>Nav-R1: Reasoning and Navigation in Embodied Scenes</papertitle>
                          <br>
                          Qingxiang Liu<sup>∗</sup>,
                          Ting Huang<sup>∗</sup>,
                          <strong>Zeyu Zhang</strong><sup>∗✝</sup>,
                          Hao Tang<sup>✉</sup>
                          <br><div class="spacer"></div>
                          <a href="https://aigeeksgroup.github.io/Nav-R1"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
                          <a href="https://arxiv.org/abs/2509.10884"><img src="https://img.shields.io/badge/arXiv-2509.10884-b31b1b?style=flat-square&logo=arxiv"></a>
                          <a href="https://github.com/AIGeeksGroup/Nav-R1"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
                          <!-- <a href="https://paperswithcode.com/paper/motion-anything-any-to-motion-generation"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                          <a href="https://huggingface.co/papers/2509.10884"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a>
                          <a href="https://github.com/AIGeeksGroup/Nav-R1#%EF%B8%8F-citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
                          <br>
                          <!-- <strong><em>Preprint</em></strong><br> -->
                          Nav-R1 is an embodied foundation model that integrates dialogue, reasoning, planning, and navigation capabilities to enable intelligent interaction and task execution in 3D environments.
                        </td>
                      </tr>


                      <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                        <td style="padding:0px;width:25%;vertical-align:middle">
                          <video src="image/3dr1.mp4" width="320" autoplay muted loop playsinline style="border-style: none;"></video>
                        </td>
                        <td style="padding:10px;width:75%;vertical-align:middle">
                            <papertitle>3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding</papertitle>
                          <br>
                          Ting Huang<sup>∗</sup>,
                          <strong>Zeyu Zhang</strong><sup>∗✝</sup>,
                          Hao Tang<sup>✉</sup>
                          <br><div class="spacer"></div>
                          <a href="https://aigeeksgroup.github.io/3D-R1"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
                          <a href="https://arxiv.org/abs/2507.23478"><img src="https://img.shields.io/badge/arXiv-2507.23478-b31b1b?style=flat-square&logo=arxiv"></a>
                          <a href="https://github.com/AIGeeksGroup/3D-R1"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
                          <!-- <a href="https://paperswithcode.com/paper/motion-anything-any-to-motion-generation"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                          <a href="https://huggingface.co/papers/2507.23478"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a>
                          <a href="https://github.com/AIGeeksGroup/3D-R1#%EF%B8%8F-citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
                          <br>
                          <!-- <strong><em>Preprint</em></strong><br> -->
                          3D-R1 is an open-source generalist model that enhances the reasoning of 3D VLMs for unified scene understanding.
                        </td>
                      </tr>
                      

                      <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                        <td style="padding:0px;width:25%;vertical-align:middle">
                          <video src="image/motionanything.mp4" width="320" autoplay muted loop playsinline style="border-style: none;"></video>
                        </td>
                        <td style="padding:10px;width:75%;vertical-align:middle">
                            <papertitle>Motion Anything: Any to Motion Generation</papertitle>
                          <br>
                          <strong>Zeyu Zhang</strong><sup>∗</sup>,
                          Yiran Wang<sup>∗</sup>,
                          Wei Mao,
                          Danning Li,
                          Akira Zhao,
                          Biao Wu,
                          Zirui Song,
                          Bohan Zhuang,
                          Ian Reid,
                          Richard Hartley
                          <br><div class="spacer"></div>
                          <a href="https://steve-zeyu-zhang.github.io/MotionAnything/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
                          <a href="https://arxiv.org/abs/2503.06955"><img src="https://img.shields.io/badge/arXiv-2503.06955-b31b1b?style=flat-square&logo=arxiv"></a>
                          <a href="https://github.com/steve-zeyu-zhang/MotionAnything"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
                          <!-- <a href="https://paperswithcode.com/paper/motion-anything-any-to-motion-generation"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
                          <a href="https://huggingface.co/papers/2503.06955"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a>
                          <a href="https://steve-zeyu-zhang.github.io/MotionAnything/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
                          <br>
                          <!-- <strong><em>Preprint</em></strong><br> -->
                          Motion Anything advances multimodal motion generation with an Any-to-Motion framework, introducing Attention-based Mask Modeling for fine-grained control. It surpasses prior methods and introduces TMD, a large text-music-dance dataset, achieving state-of-the-art results.
                        </td>
                      </tr>
           
           
                   </tbody></table>
                 <!-- </div></details> -->


                  
                  <hr>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:0px;width:100%;vertical-align:middle">
                              <br>
                              <heading>Research Experience</heading>
                              <br><br>
                              <!-- Toggle Button -->
                              <button id="toggleResearchButton" class="toggle-btn" onclick="toggleResearch()">Show all</button>
                              <br>

                <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
      
                  <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/giga.png" width="70" style="border-style: none;">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Research Intern</papertitle>
                    <br>
                    <a href="https://gigaai.cc/">GigaAI</a>
                    <br>
                    <em>Dec 2024 - Present</em>
                    <br>
                    3D generation, spatial intelligence, and world model, working with <a href="http://www.zhengzhu.net/">Dr. Zheng Zhu</a> (GigaAI).
                    </td>
                  </tr>
                </tbody></table>
      
      
                  <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/damo.png" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Research Intern</papertitle>
                    <br>
                    <a href="https://damo.alibaba.com/">Alibaba DAMO Academy</a>
                    <br>
                    <em>Oct 2024 - Present</em>
                    <br>
                    Efficient long video generation, working with Mr. Jiasheng Tang (DAMO) and <a href="https://bohanzhuang.github.io/">Prof. Bohan Zhuang</a> (ZJU, DAMO).
                    </td>
                  </tr>
                </tbody></table>
      
                  <table class="extra-research" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/zju.png" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Research Assistant</papertitle>
                    <br>
                    <a href="https://www.zju.edu.cn/">Zhejiang University</a>
                    <br>
                    <em>Aug 2024 - Jan 2025</em>
                    <br>
                    Efficient generative models, working with <a href="https://bohanzhuang.github.io/">Prof. Bohan Zhuang</a> (ZJU).
                    </td>
                  </tr>
                </tbody></table>
      
                  <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/pku.png" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Researcher</papertitle>
                    <br>
                    <a href="https://english.pku.edu.cn/">Peking University</a>
                    <br>
                    <em>July 2024 - Present</em>
                    <br>
                    Spatial intelligence and embodied AI, working with <a href="https://ha0tang.github.io/">Asst. Prof. Hao Tang</a> (PKU).
                    </td>
                  </tr>
                </tbody></table>

                <table class="extra-research" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody></tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/csail.png" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Visiting Student Researcher</papertitle>
                    <br>
                    <a href="https://www.csail.mit.edu/">MIT CSAIL</a>
                    <br>
                    <em>Jun 2024 - Aug 2024</em>
                    <br>
                    Physically compatible 3D generation in <a href="https://hcie.csail.mit.edu/">MIT CSAIL HCIE group</a>, working with <a href="https://hcie.csail.mit.edu/stefanie-mueller.html">Assoc. Prof. Stefanie Mueller</a> (MIT CSAIL) and <a href="https://sites.mit.edu/farazfaruqi/">Mr. Faraz Faruqi</a> (MIT CSAIL).
                    </td>
                  </tr>
                </tbody></table>
      
                  <table class="extra-research" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/mbzuai.jpg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Research Intern</papertitle>
                    <br>
                    <a href="https://mbzuai.ac.ae/">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</a>
                    <br>
                    <em>May 2024 - June 2024</em>
                    <br>
                    Unsupervised classification of cellular structures based on cryo-electron tomography (cryo-ET), working with <a href="https://xulabs.github.io/min-xu/">Assoc. Prof. Min Xu</a> (CMU, MBZUAI) and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a> (MBZUAI, AIML).
                    </td>
                  </tr>
                </tbody></table>
      
                  <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/latrobe.jpeg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Research Assistant</papertitle>
                    <br>
                    <a href="https://www.latrobe.edu.au/">La Trobe University</a>
                    <br>
                    <em>Apr 2024 - Present</em>
                    <br>
                    3D generation and AI for Heath, working with <a href="https://yangyangkiki.github.io/">Dr. Yang Zhao</a> (La Trobe University).
                    </td>
                  </tr>
                </tbody></table>
      
                  <table class="extra-research" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/monash.jpeg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Research Assistant</papertitle>
                    <br>
                    <a href="https://www.monash.edu/">Monash University</a>
                    <br>
                    <em>Feb 2024 - May 2024</em>
                    <br>
                    3D/4D generative learning, specifically focusing on text-guided human motion and avatar generation, working with <a href="https://users.monash.edu.au/~gholamrh/">Prof. Reza Haffari</a> (Monash University), and <a href="https://bohanzhuang.github.io/">Prof. Bohan Zhuang</a> (ZJU, Monash University).
                    </td>
                  </tr>
                </tbody></table>
      
                  <table class="extra-research" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()"></tr>
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/nci.jpeg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Research Intern</papertitle>
                    <br>
                    <a href="https://nci.org.au/">National Computational Infrastructure (NCI)</a>
                    <br>
                    <em>Feb 2023 - Jun 2023</em>
                    <br>
                    Long tail large scale multi-label text classification, working with <a href="https://nci.org.au/research/people/dr-jingbo-wang">Dr. Jingbo Wang</a> (NCI).
                    </td>
                  </tr>
                </tbody></table>
      
                  <table class="extra-research" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/aiml.jpg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Visiting Student Researcher</papertitle>
                    <br>
                    <a href="https://www.adelaide.edu.au/aiml/">Australian Institute for Machine Learning (AIML)</a>
                    <br>
                    <em>Nov 2022 - Jan 2024</em>
                    <br>
                    3D medical imaging analysis, with a particular focus on semantic segmentations of tumors, hemorrhages, and organs at risk, working with <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a> (MBZUAI, AIML), <a href="https://www.adelaide.edu.au/directory/b.zhang">Dr. Bowen Zhang</a> (AIML), <a href="https://researchers.adelaide.edu.au/profile/yutong.xie">Dr. Yutong Xie</a> (AIML), and <a href="https://scholar.google.com/citations?user=OgKU77kAAAAJ&hl=zh-CN">Dr. Qi Chen</a> (AIML).
                    </td>
                  </tr>
                </tbody></table>
      
                <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/fhmri.png" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Research Assistant</papertitle>
                    <br>
                    <a href="https://www.flinders.edu.au/health-medical-research-institute">Flinders Health and Medical Research Institute (FHMRI)</a>
                    <br>
                    <em>Nov 2022 - Present</em>
                    <br>
                    3D medical imaging analysis, particularly in the realms of 2D and 3D medical representation learning and explainable AI, working with <a href="https://scholar.google.com/citations?user=NIc4qPsAAAAJ&hl=en">Dr. Minh-Son To</a> (FHMRI).
                    </td>
                  </tr>
                </tbody></table>
      
                <table class="extra-research" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/anu.jpg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Student Researcher</papertitle>
                    <br>
                    <a href="https://www.anu.edu.au/">The Australian National University (ANU)</a>
                    <br>
                    <em>Jul 2022 - Nov 2022</em>
                    <br>
                    Diabetes diagnosis in deep learning, working with <a href="https://radiopaedia.org/users/kaspar-lewis-yaxley?lang=us">Dr. Md Zakir Hossain</a> (ANU, Curtin University, CSIRO Data61), <a href="https://scholar.google.com/citations?user=rUJ9DVAAAAAJ&hl=en">Dr. Khandaker Asif Ahmed</a> (CSIRO), <a href="https://staffportal.curtin.edu.au/staff/profile/view/rakibul-hasan-145a1046/"> Mr. Md Rakibul Hasan</a> (Curtin University, Brac University), and <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Prof. Tom Gedeon</a> (Curtin University, ANU, Óbuda University).
                    </td>
                  </tr>  
                </tbody></table>



                <hr>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:0px;width:100%;vertical-align:middle">
                              <br>
                              <heading>Education Experience</heading>
                              <br><br>
                              <!-- Toggle Button -->
                              <button id="toggleEduButton" class="toggle-btn" onclick="toggleEducation()">Show all</button>
                              <br>

                <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/anu.jpg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Bachelor of Science (Advanced) (Honours)</papertitle>
                    <br>
                    <a href="https://www.anu.edu.au/">The Australian National University (ANU)</a>
                    <br>
                    <em>Jul 2021 - Jun 2025</em>
                    <br>
                    Major: Computer Science, Minor: Mathematics, First Class Honours (H1), GPA: 6.656/7
                    </td>
                  </tr>  
                </tbody></table>
        
                <table class="extra-edu" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/ic.jpg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Visiting Student</papertitle>
                    <br>
                    <a href="https://www.imperial.ac.uk/">Imperial College London</a>
                    <br>
                    <em>Jul 2022</em>
                    <br>
                    Quantitative Sciences Research Institute (QSRI)
                    </td>
                  </tr>  
                </tbody></table>
        
                <table class="extra-edu" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/ucl.svg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Visiting Student</papertitle>
                    <br>
                    <a href="https://www.ucl.ac.uk/">University College London (UCL)</a>
                    <br>
                    <em>Jul 2022</em>
                    <br>
                    </td>
                  </tr>  
                </tbody></table>
        
                <table class="extra-edu" style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;display:none;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:0px;vertical-align:middle">
                      <img src="image/sjtu.jpeg" width="70" style="border-style: none">
                    </td>
                    <td style="padding:10px;vertical-align:middle;width:85%;">
                      <papertitle>Visiting Student</papertitle>
                    <br>
                    <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>
                    <br>
                    <em>Dec 2021 - Jan 2022</em>
                    <br>
                    </td>
                  </tr>  
                </tbody></table>




                <hr>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:0px;width:100%;vertical-align:middle">
                              <br>
                              <heading>Honors and Awards</heading>
							  <br><br>


                <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                    <td style="padding:10px;vertical-align:middle">
                      <a href="pdf/chancellors_letter_2025.pdf"><b>Chancellor's Letter of Commendation</b></a>, The Australian National University, July 2025.<br>
                      <a href="https://www.nrf.com.au/news/nrf-vacation-scholarships-2023"><b>NRF Vacation Scholarship</b></a>, NeuroSurgical Research Foundation, Oct 2023.<br>
                      <a href="https://www.flinders.edu.au/scholarships/college-of-medicine-and-public-health-summer-research-scholarship"><b>Flinders Summer Research Scholarship</b></a>, Flinders University CMPH, Nov 2022.<br>
                      <a href="https://www.scholarships.unsw.edu.au/scholarships/id/1252"><b>UNSW Science Vacation Research Scholarship</b></a>, The UNSW Sydney, Oct 2022.<br>
                    </td>
                  </tr>
                </tbody></table>



                <hr>
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                   <tbody>
                      <tr>
                         <td style="padding:0px;width:100%;vertical-align:middle">
                            <br>
                            <heading>Academic Services</heading>
              <br><br>

              <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                  <td style="padding:10px;vertical-align:middle">
                    <b>Conference Reviewer:</b> <a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>,
                    <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a> <a href="https://iclr.cc/Conferences/2026">2026</a>,
                    <a href="https://aaai.org/conference/aaai/aaai-25">AAAI 2026</a>,
                    <a href="https://acmmm2025.org/">MM 2025</a>,
                    <a href="https://2025.ijcai.org/">IJCAI 2025</a>,
                    <a href="https://conferences.miccai.org/2025/">MICCAI 2025</a>,
                    <a href="https://chi2025.acm.org/">CHI 2025</a>,
                    <a href="https://3dvconf.github.io/2026/">3DV 2026</a>,
                    <a href="https://ieeevr.org/2025/">VR 2025</a>.<br>
                  </td>
                </tr>
              </tbody></table>  



                  <hr>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:0px;width:100%;vertical-align:middle">
                              <br>
                              <heading>Talks</heading>
							  <br><br>

                            <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                              <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                                <td style="padding:10px;vertical-align:middle">
                                  <b>(09/19/2025)</b> <b>Grounding Foundation Models to the Real World</b> @ <a href="https://english.pku.edu.cn/"><b>Peking University</b></a>. Our <a href="pdf/PKU_09192025.pdf"><b>slides</b></a> and <a href="https://www.youtube.com/watch?v=3mtlb11rxrY"><b>recording</b></a> are available.<br>
                                  <b>(09/18/2025)</b> <b>Spatial Intelligence: From Virtual to Real Worlds</b> @ <a href="https://yahaha.com/"><b>Yahaha</b></a>. Our <a href="pdf/yahaha_09182025.pdf"><b>slides</b></a> and <a href="https://youtu.be/ilFf0wqPKso"><b>recording</b></a> are available.<br>
                                  <b>(07/22/2024)</b> <b>Motion Mamba: Efficient and Long Sequence Motion Generation</b> @ <a href="https://www.mihoyo.com/"><b>miHoYo</b></a>. Our <a href="https://steve-zeyu-zhang.github.io/MotionMamba/static/pdfs/Motion_Mamba_Slides_miHoYo.pdf"><b>slides</b></a> are available.<br>
                                </td>
                              </tr>
                            </tbody></table>    
                              <br><br>
                           </td>
                        </tr>
                     </tbody>
                  </table>

                  <p style="height: 40px;">&nbsp;</p><br>
               </td>
            </tr>
      </table>

      <script>
        function togglePublications() {
          const extraPubs = document.querySelectorAll('.extra-pubs');
          const button = document.getElementById('toggleButton');
          const isHidden = extraPubs[0].style.display === "none";
        
          extraPubs.forEach(row => {
            row.style.display = isHidden ? "table-row" : "none";
          });
        
          button.textContent = isHidden ? "Show less" : "Show all";
        }
        </script>


        <script>
          function toggleEducation() {
            const extraEdu = document.querySelectorAll('.extra-edu');
            const button = document.getElementById('toggleEduButton');
            const isHidden = extraEdu[0].style.display === "none";
          
            extraEdu.forEach(row => {
              row.style.display = isHidden ? "table" : "none";
            });
          
            button.textContent = isHidden ? "Show less" : "Show all";
          }
          </script>


          <script>
            function toggleResearch() {
              const extraResearch = document.querySelectorAll('.extra-research');
              const button = document.getElementById('toggleResearchButton');
              const isHidden = extraResearch[0].style.display === "none";
            
              extraResearch.forEach(row => {
                row.style.display = isHidden ? "table" : "none";
              });
            
              button.textContent = isHidden ? "Show less" : "Show all";
            }
            </script>
            


          <style>
            .toggle-btn {
              background-color: #f2f2f2; /* light gray */
              border: none;             /* no border */
              border-radius: 6px;
              padding: 6px 12px;
              font-size: 14px;
              font-weight: bold;        /* bold text */
              font-family: 'Noto Sans';
              cursor: pointer;
              transition: background-color 0.2s ease-in-out;
            }
            
            .toggle-btn:hover {
              background-color: #e0e0e0;
            }
            
            .toggle-btn:active {
              background-color: #d6d6d6;
            }
            </style>
            
          

   </body>
</html>
