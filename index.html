
<!DYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="icon" type="image/x-icon" href="image/zzhang.svg">
  <title>Zeyu Zhang</title>
  
  <meta name="author" content="Zeyu Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <style>
    .spacer {
        height: 5px;
    }
</style>
<style>
  .spacer2 {
      height: 1px;
  }
</style>
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="height: 5px;">&nbsp;</p><br>
              <p style="text-align:center">
                <name> Zeyu Zhang </name>
              </p>
              <p> Zeyu Zhang is an undergraduate researcher advised by <a href="http://users.cecs.anu.edu.au/~hartley/">Prof. Richard Hartley</a> and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a>. He is also a visiting student researcher at MIT CSAIL, working with <a href="https://hcie.csail.mit.edu/stefanie-mueller.html">Assoc. Prof. Stefanie Mueller</a>. He is an incoming research assistant at USC, working with <a href="https://yuewang.xyz/"> Asst. Prof. Yue Wang</a>. His research interests are rooted in computer vision, focusing on generative 3D modeling and AI for health. Specifically, he is dedicated to advancing efficient and high-quality motion and avatar generation, as well as 3D medical imaging segmentation and representation learning. With extensive experience across multiple research disciplines, Zeyu actively explores cutting-edge advancements in both the foundational and applied aspects of artificial intelligence. He also collaborates closely with <a href="https://bohanzhuang.github.io/">Prof. Bohan Zhuang</a> (ZJU), <a href="https://ha0tang.github.io/">Asst. Prof. Hao Tang</a> (PKU), <a href="https://yangyangkiki.github.io/index.html">Dr. Yang Zhao</a> (La Trobe), <a href="https://scholar.google.com/citations?user=NIc4qPsAAAAJ&hl=en">Dr. Minh-Son To</a> (FHMRI), and many others. <b style="color: red;">Zeyu is actively seeking PhD, RA, and internship in the US.</b>
              </p>                 
              <p style="text-align:center">
                <!-- <a href=""><img src="image/homepage.svg" style="height: 20px;vertical-align:middle;"></a> &nbsp; -->
                <a href="https://github.com/steve-zeyu-zhang"><img src="image/github.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                <a href="https://www.linkedin.com/in/steve-zeyu-zhang/"><img src="image/linkedin.svg" style="height: 30px;vertical-align:middle;"></a> &nbsp
                <a href="https://scholar.google.com/citations?user=CbsajL8AAAAJ"><img src="image/googlescholar_new.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                <a href="https://www.youtube.com/@SteveZeyuZhang"><img src="image/youtube.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                <a href="https://x.com/SteveZeyuZhang"><img src="image/x.svg" style="height: 23px;vertical-align:middle;"></a> &nbsp
                <a href="https://www.instagram.com/stevezeyuzhang"><img src="image/instagram.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                <a href="mailto:steve.zeyu.zhang@outlook.com"><img src="image/email.svg" style="height: 25px;vertical-align:middle;"></a> &nbsp
                <a href="pdf/zeyu_zhang_resume.pdf"><img src="image/resume.svg" style="height: 22px;vertical-align:middle;"></a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:85%;max-width:85%" alt="profile photo" src="image/zeyu.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:5px 20px;width:100%;vertical-align:middle;">
              <h2 style="margin: 0;">News</h2>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:10px;vertical-align:middle">
              <b>(10/14/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/MedDet/"><b>MedDet</b></a> has been accepted for an <b style="color: red;">oral presentation</b> at <a href="https://ieeebibm.org/BIBM2024/"><b>BIBM 2024</b></a>!<br>
              <b>(07/19/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/MotionAvatar/"><b>Motion Avatar</b></a> has been accepted to <a href="https://bmvc2024.org/"><b>BMVC 2024</b></a>!<br>
              <b>(07/02/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><b>Motion Mamba</b></a> has been accepted to <a href="https://eccv2024.ecva.net/"><b style="color: OrangeRed;">ECCV 2024</b></a>!<br>
              <b>(06/18/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/JointViT/"><b>JointViT</b></a> has been selected as <b style="color: red;">oral presentation</b> at <a href="https://miua2024.github.io/"><b>MIUA 2024</b></a>!<br>
              <b>(05/14/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/JointViT/"><b>JointViT</b></a> has been accepted to <a href="https://miua2024.github.io/"><b>MIUA 2024</b></a>!<br>
              <b>(03/13/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><b>Motion Mamba</b></a> has been featured in <a href="https://twitter.com/_akhaliq/status/1767750847239262532"><b>Daily Papers</b></a>!<br>
              <b>(02/10/2024)</b> &#127881; Our paper <a href="https://steve-zeyu-zhang.github.io/SegReg/"><b>SegReg</b></a> has been accepted to <a href="https://biomedicalimaging.org/2024/"><b>ISBI 2024</b></a>!
            </td>
          </tr>
        </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Publications</h2>
              <p>
                Selected publications are <span class="highlight">highlighted</span>. (<sup>∗</sup>Equal contribution. <sup>✝</sup>Project lead. <sup>✉</sup>Corresponding author.)
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          
          

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/motionanything.gif" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Motion Anything: One Prompt for Multimodal Motion and Avatar Generation</papertitle>
              <br>
              <strong>Zeyu Zhang</strong><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/yiran-wang-101739246">Yiran Wang</a><sup>∗</sup>,
              <a href="https://wei-mao-2019.github.io/home/">Wei Mao</a>,
              <a href="https://www.linkedin.com/in/danning-li-448039229/">Danning Li</a>,
              <a href="https://www.linkedin.com/in/akira-zhao/">Rui Zhao</a>,
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a>,
              <a href="https://ziruisongbest.github.io/">Zirui Song</a>,
              <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a>,
              <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Ian Reid</a>,
              <a href="https://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/MotionAnything/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <!-- <a href="https://arxiv.org/abs/2411.06481"><img src="https://img.shields.io/badge/arXiv-2411.06481-b31b1b?style=flat-square&logo=arxiv"></a> -->
              <a href="https://github.com/steve-zeyu-zhang/MotionAnything"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <!-- <a href="https://paperswithcode.com/paper/kmm-key-frame-mask-mamba-for-extended-motion"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a> -->
              <!-- <a href="https://huggingface.co/papers/2411.06481"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a> -->
              <!-- <a href="https://steve-zeyu-zhang.github.io/KMM/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>Preprint</em></strong><br>
              Motion Anything introduces a unified method for generating 4D avatars with text, music, and dance, leveraging adaptive transformers, selective rigging, and a new Text-Music-Dance dataset for multimodal tasks.
            </td>
          </tr>



          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/kmm.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>KMM: Key Frame Mask Mamba for Extended Motion Generation</papertitle>
              <br>
              <strong>Zeyu Zhang</strong><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/hang-gao-725986307/?originalSubdomain=au">Hang Gao</a><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/akideliu/">Akide Liu</a>,
              <a href="https://chenqi008.github.io/">Qi Chen</a>,
              <a href="https://github.com/Chenfeng1271">Feng Chen</a>,
              <a href="https://www.linkedin.com/in/yiran-wang-101739246/">Yiran Wang</a>,
              <a href="https://www.linkedin.com/in/danning-li-448039229/">Danning Li</a>,
              <a href="https://www.linkedin.com/in/akira-zhao/">Rui Zhao</a>,
              <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Ian Reid</a>,
              <a href="https://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a>,
              <a href="https://ha0tang.github.io/">Hao Tang</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/KMM/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2411.06481"><img src="https://img.shields.io/badge/arXiv-2411.06481-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/KMM"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/kmm-key-frame-mask-mamba-for-extended-motion"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://huggingface.co/papers/2411.06481"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a>
              <a href="https://steve-zeyu-zhang.github.io/KMM/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              KMM addresses memory decay and multimodal fusion in motion generation, introducing key frame masking and contrastive learning, achieving state-of-the-art results on BABEL with superior efficiency and alignment.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/infinimotion.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>InfiniMotion: Mamba in Mamba for Long Motion Generation</papertitle>
              <br>
              <strong>Zeyu Zhang</strong><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/akideliu/">Akide Liu</a>,
              <a href="https://chenqi008.github.io/">Qi Chen</a>,
              <a href="https://github.com/Chenfeng1271">Feng Chen</a>,
              <a href="https://www.linkedin.com/in/yiran-wang-101739246/">Yiran Wang</a>,
              <a href="https://www.linkedin.com/in/danning-li-448039229/">Danning Li</a>,
              <a href="https://ling-shao.github.io/">Ling Shao</a>,
              <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Ian Reid</a>,
              <a href="https://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a>,
              <a href="https://ha0tang.github.io/">Hao Tang</a><sup>✉</sup>,
              <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/InfiniMotion/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2407.10061"><img src="https://img.shields.io/badge/arXiv-2407.10061-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/InfiniMotion"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/infinimotion-mamba-boosts-memory-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://huggingface.co/papers/2407.10061"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a>
              <a href="https://steve-zeyu-zhang.github.io/InfiniMotion/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              InfiniMotion introduces a novel mamba-in-mamba architecture with memory updates and a similarity-based masking strategy, achieving 15% FID improvement on BABEL for robust, long-sequence motion generation.
            </td>
          </tr>


        <tr onmouseout="ld_stop()" onmouseover="ld_start()" bgcolor="#ffffd0">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/motionmamba.gif" width="320" height="180" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Motion Mamba: Efficient and Long Sequence Motion Generation</papertitle>
              <br>
              <strong>Zeyu Zhang</strong><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/akideliu/">Akide Liu</a><sup>∗</sup>,
              <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Ian Reid</a>,
              <a href="http://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a>,
              <a href="https://bohanzhuang.github.io/">Bohan Zhuang</a>,
              <a href="https://ha0tang.github.io/">Hao Tang</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2403.07487"><img src="https://img.shields.io/badge/arXiv-2403.07487-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/MotionMamba"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/motion-mamba-efficient-and-long-sequence"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://huggingface.co/papers/2403.07487"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-555555?style=flat-square"></a>
              <a href="https://steve-zeyu-zhang.github.io/MotionMamba/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://eccv2024.ecva.net/"><strong><em style="color: OrangeRed;">ECCV 2024</em></strong><br></a>
              Human motion generation is a key goal in generative computer vision, and we propose Motion Mamba, a model using state space models (SSMs) with Hierarchical Temporal Mamba (HTM) and Bidirectional Spatial Mamba (BSM) blocks, achieving up to 50% FID improvement and 4x speedup on HumanML3D and KIT-ML datasets, showcasing efficient and high-quality long sequence motion modeling.
            </td>
          </tr>


        </tbody></table>

      
          <details>
            <summary><b>More Publications</b></summary><div>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>  


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/meddet.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>MedDet: Generative Adversarial Distillation for Efficient Cervical Disc Herniation Detection</papertitle>
              <br>
              <strong>Zeyu Zhang</strong><sup>∗</sup>,
              <a href="https://www.researchgate.net/scientific-contributions/Nengmin-Yi-2249685869">Nengmin Yi</a><sup>∗</sup>,
              <a href="https://dblp.org/pid/205/0179.html">Shengbo Tan</a><sup>∗</sup>,
              <a href="https://ieeexplore.ieee.org/author/37087137422">Ying Cai</a><sup>✉</sup>,
              <a href="https://www.researchgate.net/scientific-contributions/Yi-Yang-2208407378">Yi Yang</a>,
              <a href="https://www.researchgate.net/profile/Lei-Xu-106">Lei Xu</a>,
              <a href="https://pubmed.ncbi.nlm.nih.gov/?term=Li+Q&cauthor_id=38902109">Qingtai Li</a>,
              <a href="https://scholar.google.com.hk/citations?user=kCtQkrkAAAAJ">Zhang Yi</a>,
              <a href="https://ieeexplore.ieee.org/author/37085795653">Daji Ergu</a>,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/MedDet/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2409.00204"><img src="https://img.shields.io/badge/arXiv-2409.00204-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/MedDet"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href=""><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/MedDet/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://ieeebibm.org/BIBM2024/"><strong><em>BIBM 2024</em></strong> <strong style="color: red;"><em>Oral</em></strong><br></a>
              Cervical disc herniation (CDH) is a common disorder needing expert analysis. Current automated detection methods face challenges: high computational demands and MRI noise. We propose MedDet for efficient detection, leveraging knowledge distillation, generative adversarial training, and nmODE<sup>2</sup>. Our model improves mAP by 5%, reduces parameters by 67.8%, and speeds inference fivefold.
            </td>
          </tr>

          
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/motionavatar.gif" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</papertitle>
              <br>
              <strong>Zeyu Zhang</strong><sup>∗</sup>,
              <a href="https://github.com/u7079256">Yiran Wang</a><sup>∗</sup>,
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/shuo-chen-7747a7246/">Shuo Chen</a>,
              <a href="https://github.com/Tiooo111">Zhiyuan Zhang</a>,
              <a href="https://github.com/gekelly">Shiya Huang</a>,
              <a href="https://zwbx.github.io/">Wenbo Zhang</a>,
              <a href="https://mengf1.github.io/">Meng Fang</a>,
              <a href="https://profiles.uts.edu.au/Ling.Chen">Ling Chen</a>,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/MotionAvatar/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://arxiv.org/abs/2405.11286"><img src="https://img.shields.io/badge/arXiv-2405.11286-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/MotionAvatar"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/motion-avatar-generate-human-and-animal"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/MotionAvatar/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://bmvc2024.org/"><strong><em>BMVC 2024</em></strong><br></a>
              Our paper introduces a novel agent-based approach called Motion Avatar for generating customizable human and animal 3D avatars with motions via text queries, coordinated by an LLM planner, and supported by the new Zoo-300K animal motion dataset.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/jointvit.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>JointViT: Modeling Oxygen Saturation Levels with Joint Supervision on Long-Tailed OCTA</papertitle>
              <br>
              <strong>Zeyu Zhang</strong>,
              <a href="https://www.linkedin.com/in/xuyin-q-29672524a/">Xuyin Qi</a>,
              <a href="https://www.linkedin.com/in/mingxi-chen-4b57562a1/">Mingxi Chen</a>,
              <a href="https://github.com/lgX1123">Guangxi Li</a>,
              <a href="https://www.flinders.edu.au/people/ryan.pham">Ryan Pham</a>,
              <a href="https://www.flinders.edu.au/people/ayub.qassim">Ayub Qassim</a>,
              <a href="https://www.linkedin.com/in/ella-berry-a2a3aab4/">Ella Berry</a>,
              <a href="https://researchers.adelaide.edu.au/profile/zhibin.liao">Zhibin Liao</a>,
              <a href="https://researchnow.flinders.edu.au/en/persons/owen-siggs-2">Owen Siggs</a>,
              <a href="https://researchers.adelaide.edu.au/profile/robert.mclaughlin">Robert Mclaughlin</a>,
              <a href="https://www.flinders.edu.au/people/jamie.craig">Jamie Craig</a>,
              <a href="https://www.flinders.edu.au/people/minhson.to">Minh-Son To</a>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/JointViT/"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://doi.org/10.1007/978-3-031-66955-2_11"><img src="https://img.shields.io/badge/DOI-10.1007%2F978--3--031--66955--2__11-fcb520?style=flat-square&logo=doi"></a>
              <a href="https://arxiv.org/abs/2404.11525"><img src="https://img.shields.io/badge/arXiv-2404.11525-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/JointViT"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/jointvit-modeling-oxygen-saturation-levels"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/JointViT/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://miua2024.github.io/"><strong><em>MIUA 2024</em></strong> <strong style="color: red;"><em>Oral</em></strong><br></a>
              Our paper introduces JointViT, a Vision Transformer model with a novel joint loss function and balancing augmentation technique that significantly improves the accuracy of diagnosing sleep-related disorders using OCTA, achieving up to a 12.28% accuracy improvement.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/segreg.gif" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>SegReg: Segmenting OARs by Registering MR Images and CT Annotations</papertitle>
              <br>
              <strong>Zeyu Zhang</strong>,
              <a href="https://www.linkedin.com/in/xuyin-q-29672524a/">Xuyin Qi</a>,
              <a href="https://www.adelaide.edu.au/directory/b.zhang">Bowen Zhang</a>,
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a>,
              <a href="https://iconcancercentre.com.au/doctor/hien-le">Hien Le</a>,
              <a href="https://www.linkedin.com/in/bora-jeong-5a3177231/">Bora Jeong</a>,
              <a href="https://researchers.adelaide.edu.au/profile/zhibin.liao">Zhibin Liao</a>,
              <a href="https://www.adelaide.edu.au/directory/yunxiang.liu">Yunxiang Liu</a>,
              <a href="https://researchers.adelaide.edu.au/profile/johan.verjans">Johan Verjans</a>,
              <a href="https://www.flinders.edu.au/people/minhson.to">Minh-Son To</a>,
              <a href="http://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/SegReg"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://doi.org/10.1109/ISBI56570.2024.10635437"><img src="https://img.shields.io/badge/DOI-10.1109%2FISBI56570.2024.10635437-fcb520?style=flat-square&logo=doi">
              <a href="https://arxiv.org/abs/2311.06956"><img src="https://img.shields.io/badge/arXiv-2311.06956-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://openreview.net/forum?id=rC8bmJoOOTC"><img src="https://img.shields.io/badge/OpenReview-8c1b13?style=flat-square"></a>
              <a href="https://github.com/steve-zeyu-zhang/SegReg"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/segreg-segmenting-oars-by-registering-mr"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/SegReg/webpage/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://biomedicalimaging.org/2024/"><strong><em>ISBI 2024</em></strong><br></a>
              To improve the accuracy and efficiency of organ at risk (OAR) segmentation in radiotherapy, we propose SegReg, a method that combines CT and MRI using Elastic Symmetric Normalization, outperforming traditional CT-only methods by 16.78% in mDSC and 18.77% in mIoU.
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()"">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/tta.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Thin-Thick Adapter: Segmenting Thin Scans Using Thick Annotations</papertitle>
              <br>
              <strong>Zeyu Zhang</strong>,
              <a href="https://www.adelaide.edu.au/directory/b.zhang">Bowen Zhang</a>,
              <a href="https://www.nrf.com.au/news/rotem-latest-research">Abhiram Hiwase</a>,
              <a href="https://www.researchgate.net/scientific-contributions/Feng-Chen-2262634351">Feng Chen</a>,
              <a href="https://www.linkedin.com/in/akideliu/">Akide Liu</a>,
              <a href="https://jonesradiology.com.au/barras-christen">Christen Barras</a>,
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a>,
              <a href="https://www.dradamwells.com.au/">Adam Wells</a>,
              <a href="https://www.thefrontiersconference.org/daniel-ellis">Daniel Ellis</a>,
              <a href="https://www.adelaide.edu.au/directory/benjamin.reddi">Benjamin Reddi</a>,
              <a href="https://www.linkedin.com/in/andrew-burgan-572334108/?originalSubdomain=au">Andrew Burgan</a>,
              <a href="https://www.flinders.edu.au/people/minhson.to">Minh-Son To</a>,
              <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Ian Reid</a>,
              <a href="http://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a>,
              <a href="https://scholar.google.com/citations?user=ddDL9HMAAAAJ&hl=zh-CN">Yutong Xie</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://openreview.net/forum?id=NF5uhYkI9C"><img src="https://img.shields.io/badge/OpenReview-8c1b13?style=flat-square"></a>
              <a href="https://steve-zeyu-zhang.github.io/ThinThickAdapter/website/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Medical imaging segmentation is critical for medical analysis, predominantly using thicker CT slices due to the scarcity of annotated thin slices, so we propose segmenting thin scans with thicker slice annotations, introduce the CQ500-Thin dataset, and present the Thin-Thick Adapter to bridge domain gaps, significantly improving segmentation performance.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/diabetes.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>DiabetesNet: A Deep Learning Approach to Diabetes Diagnosis</papertitle>
              <br>
              <strong>Zeyu Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=rUJ9DVAAAAAJ&hl=en">Khandaker Asif Ahmed</a>,
              <a href="https://staffportal.curtin.edu.au/staff/profile/view/rakibul-hasan-145a1046/">Md Rakibul Hasan</a>,
              <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Tom Gedeon</a>,
              <a href="https://radiopaedia.org/users/kaspar-lewis-yaxley?lang=us">Md Zakir Hossain</a>
              <br><div class="spacer"></div>
              <a href="https://steve-zeyu-zhang.github.io/DiabetesDiagnosis"><img src="https://img.shields.io/badge/Website-Demo-fedcba?style=flat-square"></a>
              <a href="https://doi.org/10.1007/978-981-97-5937-8_8"><img src="https://img.shields.io/badge/DOI-10.1007%2F978--981--97--5937--8__8-fcb520?style=flat-square&logo=doi">
              <a href="https://arxiv.org/abs/2403.07483"><img src="https://img.shields.io/badge/arXiv-2403.07483-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/steve-zeyu-zhang/DiabetesDiagnosis"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/a-deep-learning-approach-to-diabetes"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/static/scholar.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://aciids.pwr.edu.pl/2024/"><strong><em>ACIIDS 2024</em></strong><br></a>
              We propose a non-invasive diabetes diagnosis method using a Back Propagation Neural Network with batch normalization, addressing class imbalance and improving performance over traditional methods, achieving high accuracy on multiple datasets.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td colspan="2" style="padding:0;"><hr></td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/projectedex.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>ProjectedEx: Enhancing Generation in Explainable AI for Prostate Cancer</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/xuyin-q-29672524a/">Xuyin Qi</a><sup>∗</sup>,
              <strong>Zeyu Zhang</strong><sup>∗✝</sup>,
              <a href="https://www.linkedin.com/in/aaron-berliano-handoko-235406187/">Aaron Berliano Handoko</a><sup>∗</sup>,
              Huazhan Zheng,
              Mingxi Chen,
              <a href="https://scholar.google.com/citations?user=9vRcgJwAAAAJ&hl=en">Ta Duc Huy</a>,
              <a href="https://researchers.adelaide.edu.au/profile/vuminhhieu.phan">Vu Minh Hieu Phan</a>,
              Lei Zhang,
              Linqi Cheng,
              Shiyu Jiang,
              Zhiwei Zhang,
              <a href="https://scholar.google.com/citations?user=HvWTE0IAAAAJ&hl=zh-CN">Zhibin Liao</a>,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a><sup>✉</sup>,
              <a href="https://scholar.google.com/citations?user=NIc4qPsAAAAJ&hl=en">Minh-Son To</a>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Prostate cancer diagnosis benefits from MRI and AI advancements. We propose ProjectedEx, a generative framework with multiscale feedback, enhancing interpretability and lesion classification while addressing challenges in medical imaging complexity.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td colspan="2" style="padding:0;"><hr></td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/heterogeneity.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>CT Heterogeneity and Dose Distribution Patterns in Block and Ring Regions Improved the Prediction of Radiation Pneumonitis</papertitle>
              <br>
              Yuyu Liu, 
              Wang Li, 
              Fangfang Yang, 
              Yali Wang,
              <strong>Zeyu Zhang</strong>,
              Biao Wu, 
              Gao Yanping, 
              Han Bai, 
              Wenbing Lv
              <!-- <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>In Submission (EMBC 2025)</em></strong><br>
              
This study analyzed 251 lung cancer patients to examine CT heterogeneity and dose distribution's impact on radiation pneumonitis (RP). Using radiomics features and dose-volume histogram parameters across block and ring regions, seven machine learning models were applied. Results showed multi-modal features outperforming DVH, with ring regions (40–50 Gy) achieving the highest AUC (0.977). This supports personalized treatment planning.
            </td>
          </tr>



          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/gnn.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>A GNN-based Fraud Detection Approach against Heterophily Inconsistencies</papertitle>
              <br>
              Wenxin Zhang, 
              Jingxing Zhong, 
              <strong>Zeyu Zhang</strong>,
              Lingfei Ren,
              Cuicui Luo
              <!-- <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>In Submission (IJCAI 2025)</em></strong><br>
              Graph-based fraud detection is vital yet challenged by heterophilic inconsistencies, often overlooking semantic nuances. We propose HIGNN, leveraging distinct semantic patterns in heterophilic connections to enhance fraud detection robustness, validated on real-world datasets.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/bias.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Bias and Toxicity in Large Language Model Role-Playing</papertitle>
              <br>
              Jinman Zhao, 
              Zifan Qian, 
              Linbo Cao, 
              Yining Wang, 
              Yitian Ding, 
              Yulan Hu,
              <strong>Zeyu Zhang</strong>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2409.13979"><img src="https://img.shields.io/badge/arXiv-2409.13979-b31b1b?style=flat-square&logo=arxiv"></a>
              <!-- <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>Preprint</em></strong><br>
              Role-play in LLMs enhances contextual responses and reasoning across benchmarks but introduces risks, as adopting diverse roles increases susceptibility to biased or harmful outputs on sensitive evaluations.
            </td>
          </tr>



          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/gamed-snake.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>GAMED-Snake: Gradient-aware Adaptive Momentum Evolution Deep Snake Model for Multi-organ Segmentation</papertitle>
              <br>
              Ruicheng Zhang,
              Haowei Guo,
              <strong>Zeyu Zhang</strong>,
              Puxin Yan,
              Shen Zhao
              <!-- <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>In Submission (ICME 2025)</em></strong><br>
              The Gradient-aware Adaptive Momentum Evolution Deep Snake (GAMED-Snake) model advances multi-organ segmentation, achieving a 2% mDice improvement via novel gradient-based learning, adaptive momentum evolution, and dynamic boundary alignment innovations.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/fdg-diff.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>FDG-Diff: Frequency-Domain-Guided Diffusion Framework for Compressed Hazy Image Restoration</papertitle>
              <br>
              Ruicheng Zhang,
              Kanghui Tian,
              <strong>Zeyu Zhang</strong>,
              Qixiang Liu,
              Zhi Jin
              <!-- <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>In Submission (ICME 2025)</em></strong><br>
              This study introduces FDG-Diff, a frequency-domain-guided framework addressing joint haze degradation and JPEG compression. Key innovations include HFCM for detail restoration and DADTP for adaptive region-specific enhancement, achieving superior dehazing performance.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/class-centric.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Class-Centric Semi-supervised Vision Transformers</papertitle>
              <br>
              Hongyang He,
              Haochen You,
              <strong>Zeyu Zhang</strong>,
              Hongyang Xie,
              Boyang Fu,
              Guodong Shen,
              Victor Sanchez
              <!-- <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>In Submission (ICME 2025)</em></strong><br>
              CLASST, a framework for Semi-Supervised Vision Transformers, addresses class imbalance by dynamically enhancing minority class representation using learnable centers and adaptive contrastive loss, achieving state-of-the-art results on imbalanced datasets.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/advmark.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>AdvMark: Robust Image Watermarking via Two Stage Adversarial Enhancement</papertitle>
              <br>
              Jiahui Chen, 
              Zehang Deng,
              <strong>Zeyu Zhang</strong>,
              Chaoyang Li, 
              Lianchen Jia, 
              Lifeng Sun
              <!-- <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>In Submission (CVPR 2025)</em></strong><br>
              AdvMark introduces a two-stage fine-tuning strategy for robust watermarking, achieving up to 46% accuracy improvement against advanced attacks while ensuring superior image quality through constrained loss and quality-aware early stopping.
            </td>
          </tr>



          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/diffumural.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>DiffuMural: Diffusion Model for Dunhuang Murals Restoration based on Multi-scale Convergence and Cooperative Diffusion</papertitle>
              <br>
              Puyu Han,
              Jiaju Kang,
              Yuhang Pan,
              Erting Pan,
              Qunchao Jin,
              Juntao Jiang,
              <strong>Zeyu Zhang</strong>,
              Zhichen Liu, 
              Luqi Gong
              <!-- <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>In Submission (CVPR 2025)</em></strong><br>
              DiffuMural combines multi-scale convergence, collaborative diffusion, and cyclic loss, excelling in ancient mural restoration with unmatched detail, style coherence, and cultural preservation, surpassing SOTA methods across comprehensive metrics.
            </td>
          </tr>



          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/ocrt.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad</papertitle>
              <br>
              Luyao Tang,
              Chaoqi Chen,
              Yuxuan Yuan,
              <strong>Zeyu Zhang</strong>,
              Yue Huang,
              Kun Zhang
              <!-- <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>In Submission (CVPR 2025)</em></strong><br>
              Foundation models struggle with distribution shifts and weak supervision. We propose OCRT, a framework extracting high-level concepts and relations, enhancing SAM and CLIP generalizability in diverse tasks.
            </td>
          </tr>



          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/epdd-yolo.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>EPDD-YOLO: An Efficient Benchmark for Pavement Damage Detection Based on Mamba-YOLO</papertitle>
              <br>
              Shipeng Luo,
              Yuxin Zhang,
              <strong>Zeyu Zhang</strong>,
              Binhua Guo,
              Junbo Jacob Lian,
              Hui Jiang,
              Shun Zou,
              Wei Wang
              <!-- <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2501.01392"><img src="https://img.shields.io/badge/arXiv-2501.01392-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/projectedex-enhancing-generation-in"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/Richardqiyi/ProjectedEx/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a> -->
              <br>
              <strong><em>In Submission (Measurement)</em></strong><br>
              EPDD-YOLO enhances pavement damage detection with advanced augmentation, architectural improvements, and the EPDD dataset, achieving 0.873 precision and real-time inference at 198 FPS on challenging benchmarks.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/segkan.jpg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>SegKAN: High-Resolution Medical Image Segmentation with Long-Distance Dependencies</papertitle>
              <br>
              <a href="https://github.com/goblin327">Shengbo Tan</a>,
              <a href="https://github.com/Yizhao-dong">Rundong Xue</a>,
              Shipeng Luo,
              <strong>Zeyu Zhang</strong><sup>✝</sup>,
              Xinran Wang,
              Lei Zhang,
              <a href="https://ieeexplore.ieee.org/author/37085795653">Daji Ergu</a>,
              Zhang Yi,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a>,
              <a href="https://ieeexplore.ieee.org/author/37087137422">Ying Cai</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2412.19990"><img src="https://img.shields.io/badge/arXiv-2412.19990-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/goblin327/segkan"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/segkan-high-resolution-medical-image"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/goblin327/SegKAN/blob/main/README.md#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Hepatic vessel segmentation faces noise and fragmentation challenges. We propose SegKAN, enhancing image embedding and spatial-temporal relationships in Vision Transformers, achieving a 1.78% Dice score improvement on a hepatic vessel dataset.
            </td>
          </tr>

        
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/hazard.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Hazards in Daily Life? Enabling Robots to Proactively Detect and Resolve Anomalies</papertitle>
              <br>
              <a href="https://ziruisongbest.github.io/">Zirui Song</a>,
              Guangxian Ouyang,
              Meng Fang,
              Hongbin Na,
              Zijing Shi,
              Zhenhao Chen,
              Yujie Fu,
              <strong>Zeyu Zhang</strong>,
              Shiyu Jiang,
              Miao Fang,
              Ling Chen,
              Xiuying Chen<sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2411.00781"><img src="https://img.shields.io/badge/arXiv-2411.00781-b31b1b?style=flat-square&logo=arxiv"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Household robots struggle to detect hazards. We propose anomaly scenario generation using multi-agent brainstorming and 3D simulations, enhancing robotic skills in hazard detection, hygiene management, and child safety through diverse environments.
            </td>
          </tr>

          
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/lung_ca_survey.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Medical AI for Early Detection of Lung Cancer: A Survey</papertitle>
              <br>
              <a href="https://github.com/CaiGuoHui123">Guohui Cai</a>,
              <a href="https://ieeexplore.ieee.org/author/37087137422">Ying Cai</a><sup>✉</sup>,
              <strong>Zeyu Zhang</strong><sup>✝</sup>,
              <a href="https://scholar.google.com/citations?user=-RBi2JcAAAAJ&hl=en">Yuanzhouhan Cao</a>,
              <a href="https://dblp.org/pid/65/6292.html">Lin Wu</a>,
              <a href="https://ieeexplore.ieee.org/author/37085795653">Daji Ergu</a>,
              <a href="https://researchers.adelaide.edu.au/profile/zhibin.liao">Zhinbin Liao</a>,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2410.14769"><img src="https://img.shields.io/badge/arXiv-2410.14769-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detection"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/medical-ai-for-early-detection-of-lung-cancer"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/CaiGuoHui123/Awesome-Lung-Cancer-Detection/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Deep learning has revolutionized pulmonary nodule analysis, surpassing traditional methods in detection, segmentation, and classification. This review highlights recent advancements, including CNNs, RNNs, GANs, and ensemble models, improving lung cancer diagnosis.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/msdet.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>MSDet: Receptive Field Enhanced Multiscale Detection for Tiny Pulmonary Nodule</papertitle>
              <br>
              <a href="https://github.com/CaiGuoHui123">Guohui Cai</a>,
              <a href="https://ieeexplore.ieee.org/author/37087137422">Ying Cai</a><sup>✉</sup>,
              <strong>Zeyu Zhang</strong><sup>✝</sup>,
              <a href="https://ieeexplore.ieee.org/author/37085795653">Daji Ergu</a>,
              <a href="https://scholar.google.com/citations?user=-RBi2JcAAAAJ&hl=en">Yuanzhouhan Cao</a>,
              <a href="https://dl.acm.org/profile/99661184796">Binbin Hu</a>,
              <a href="https://researchers.adelaide.edu.au/profile/zhibin.liao">Zhinbin Liao</a>,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2409.14028"><img src="https://img.shields.io/badge/arXiv-2409.14028-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/CaiGuoHui123/MSDet"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/msdet-receptive-field-enhanced-multiscale"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/CaiGuoHui123/MSDet#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Pulmonary nodules are vital for early lung cancer diagnosis but challenging to detect. Our proposed MSDet network, with ERD, PCAM, and TODB strategies, achieves state-of-the-art results on LUNA16, improving mAP by 8.8%.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/esa.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>ESA: Annotation-Efficient Active Learning for Semantic Segmentation</papertitle>
              <br>
              <a href="https://dblp.org/pid/256/0342.html">Jinchao Ge</a>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=gSEw8EsAAAAJ&hl=en">Minh Hieu Phan</a>,
              <a href="https://www.linkedin.com/in/bowen-zhang-a7403095/">Bowen Zhang</a>,
              <a href="https://www.linkedin.com/in/akideliu/">Akide Liu</a>,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2408.13491"><img src="https://img.shields.io/badge/arXiv-2408.13491-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/jinchaogjc/ESA/"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/esa-annotation-efficient-active-learning-for"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/jinchaogjc/ESA/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Active learning improves annotation efficiency by selecting the most informative samples for labeling. We propose Entity-Superpixel Annotation (ESA), an efficient strategy using a mask proposal network and superpixel grouping. Our method reduces click cost by 98% and boosts performance by 1.71%, outperforming pixel-based methods with only 40 clicks per image.
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/segstitch.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>SegStitch: Multidimensional Transformer for Robust and Efficient Medical Imaging Segmentation</papertitle>
              <br>
              <a href="https://github.com/goblin327">Shengbo Tan</a>,
              <strong>Zeyu Zhang</strong><sup>✝</sup>,
              <a href="https://ieeexplore.ieee.org/author/37087137422">Ying Cai</a><sup>✉</sup>,
              <a href="https://ieeexplore.ieee.org/author/37085795653">Daji Ergu</a>,
              <a href="https://dblp.org/pid/65/6292.html">Lin Wu</a>,
              <a href="https://dl.acm.org/profile/99661184796">Binbin Hu</a>,
              <a href="https://www.researchgate.net/search/researcher?q=Pengzhang+Yu">Pengzhang Yu</a>,
              <a href="https://yangyangkiki.github.io/">Yang Zhao</a>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2408.00496"><img src="https://img.shields.io/badge/arXiv-2408.00496-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/goblin327/SegStitch"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/segstitch-multidimensional-transformer-for"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/goblin327/SegStitch/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Medical imaging segmentation, crucial for lesion analysis, has seen advances with transformers in 3D segmentation. Despite their scalability, transformers struggle with local features and complexity. We propose SegStitch, combining transformers with denoising ODE blocks, improving mDSC by up to 11.48% and reducing parameters by 36.7%, promising real-world clinical adaptation.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/lowrank.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Sine Activated Low-Rank Matrices for Parameter Efficient Learning</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/yiping-ji-111b97142/">Yiping Ji</a><sup>∗</sup>,
              <a href="https://researchers.adelaide.edu.au/profile/hemanth.saratchandran">Hemanth Saratchandran</a><sup>∗</sup>,
              <a href="https://www.linkedin.com/in/camerondgordon">Cameron Gordon</a>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://researchers.adelaide.edu.au/profile/simon.lucey">Simon Lucey</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2403.19243"><img src="https://img.shields.io/badge/arXiv-2403.19243-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://paperswithcode.com/paper/sine-activated-low-rank-matrices-for"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://steve-zeyu-zhang.github.io/scholar/sine.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              We propose a novel theoretical framework integrating a sinusoidal function into low-rank decomposition, enhancing parameter efficiency and model accuracy across diverse neural network applications such as Vision Transformers, Large Language Models, Neural Radiance Fields, and 3D shape modeling.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/xlip.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a><sup>∗</sup>,
              <a href="https://v3alab.github.io/author/yutong-xie/">Yutong Xie</a><sup>∗</sup>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=gSEw8EsAAAAJ&hl=en">Minh Hieu Phan</a>,
              <a href="https://scholar.google.com/citations?user=OgKU77kAAAAJ&hl=zh-CN">Qi Chen</a>,
              <a href="https://profiles.uts.edu.au/Ling.Chen">Ling Chen</a>,
              <a href="http://www.qi-wu.me/">Qi Wu</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://arxiv.org/abs/2407.19546"><img src="https://img.shields.io/badge/arXiv-2407.19546-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://github.com/White65534/XLIP"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/xlip-cross-modal-attention-masked-modelling"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/White65534/XLIP/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <strong><em>Preprint</em></strong><br>
              Vision-and-language pretraining (VLP) in the medical field faces challenges with reconstructing pathological features due to data scarcity and limited use of paired/unpaired data. This paper proposes XLIP, using AttMIM and EntMLM modules, to enhance feature learning from unpaired data, achieving state-of-the-art results in medical classification tasks.
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/landmark.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>A Landmark-Based Approach for Instability Prediction in Distal Radius Fractures</papertitle>
              <br>
              <a href="https://yangyangkiki.github.io">Yang Zhao</a>,
              <a href="https://researchers.adelaide.edu.au/profile/zhibin.liao">Zhibin Liao</a>,
              <a href="https://www.adelaide.edu.au/directory/yunxiang.liu">Yunxiang Liu</a>,
              <a href="https://umcgresearch.org/w/k-d-oude-nijhuis">Koen Oude Nijhuis</a>,
              <a href="https://www.erasmusmc.nl/en/research/researchers/barvelink-britt">Britt Barvelink</a>,
              <a href="https://research.rug.nl/en/persons/jasper-prijs">Jasper Prijs</a>,
              <a href="https://www.erasmusmc.nl/en/research/researchers/colaris-joost">Joost Colaris</a>,
              <a href="https://pure.eur.nl/en/persons/mathieu-wijffels">Mathieu Wijffels</a>,
              <a href="https://www.erasmusmc.nl/en/research/researchers/reijman-m">Max Reijman</a>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://scholar.google.com/citations?user=NIc4qPsAAAAJ&hl=en">Minh-Son To</a>,
              <a href="https://www.flinders.edu.au/people/ruurd.jaarsma">Ruurd Jaarsma</a>,
              <a href="https://umcgresearch.org/w/job-doornberg">Job Doornberg</a>,
              <a href="https://researchers.adelaide.edu.au/profile/johan.verjans">Johan Verjans</a>
              <br><div class="spacer"></div>
              <a href="https://doi.org/10.1109/ISBI56570.2024.10635424"><img src="https://img.shields.io/badge/DOI-10.1109%2FISBI56570.2024.10635424-fcb520?style=flat-square&logo=doi"></a>
              <a href="https://steve-zeyu-zhang.github.io/scholar/landmark.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://biomedicalimaging.org/2024/"><strong><em>ISBI 2024</em></strong><br></a>
              Distal radius fractures (DRFs) are common and their instability assessment is crucial for treatment decisions, affecting recovery and costs. We propose a deep learning-based landmark detection method using anatomical landmarks from X-rays to measure distances and angles. These features are used in an XGBoost model for DRF instability classification, validated on a large Dutch dataset.
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/rotem.png" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>Can Rotational Thromboelastometry Rapidly Identify Theragnostic Targets in Isolated Traumatic Brain Injury?</papertitle>
              <br>
              <a href="https://www.nrf.com.au/news/rotem-latest-research">Abhiram Hiwase</a>,
              <a href="https://researchers.adelaide.edu.au/profile/christopher.ovenden">Christopher Ovenden</a>,
              <a href="https://www.linkedin.com/in/lola-kaukas-508700b5/">Lola Kaukas</a>,
              <a href="https://github.com/jinchaogjc">Mark Finnis</a>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://www.linkedin.com/in/stephanie-o-connor-health-a845b269/">Stephanie O'Connor</a>,
              <a href="https://www.linkedin.com/in/ngee-foo-2b2347166/">Ngee Foo</a>,
              <a href="https://www.adelaide.edu.au/directory/benjamin.reddi">Benjamin Reddi</a>,
              <a href="https://www.dradamwells.com.au/">Adam Wells</a>,
              <a href="https://www.thefrontiersconference.org/daniel-ellis">Daniel Ellis</a>
              <br><div class="spacer"></div>
              <a href="https://doi.org/10.1111/1742-6723.14480"><img src="https://img.shields.io/badge/DOI-10.1111%2F1742--6723.14480-fcb520?style=flat-square&logo=doi"></a>
              <a href="https://steve-zeyu-zhang.github.io/scholar/rotem.html"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://onlinelibrary.wiley.com/journal/17426723"><strong><em>EMA 2024</em></strong><br></a>
              This study evaluates the prognostic utility of ROTEM sigma in isolated traumatic brain injury (TBI). ROTEM sigma, a point-of-care assay, demonstrated faster turnaround times and comparable accuracy to standard coagulation tests in predicting head injury-related deaths. The findings suggest ROTEM sigma effectively detects coagulopathy in isolated TBI cases.
            </td>
          </tr>


          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/bhsd.svg" width="320" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
                <papertitle>BHSD: A 3D Multi-class Brain Hemorrhage Segmentation Dataset</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=Y3SBBWMAAAAJ&hl=en">Biao Wu</a>,
              <a href="https://v3alab.github.io/author/yutong-xie/">Yutong Xie</a>,
              <strong>Zeyu Zhang</strong>,
              <a href="https://github.com/jinchaogjc">Jinchao Ge</a>,
              <a href="https://radiopaedia.org/users/kaspar-lewis-yaxley?lang=us">Kaspar Yaxley</a>,
              <a href="https://au.linkedin.com/in/suzan-bahadir-57870416b">Suzan Bahadir</a>,
              <a href="http://www.qi-wu.me/">Qi Wu</a>,
              <a href="https://scholar.google.com/citations?user=ksQ4JnQAAAAJ&hl=zh-CN">Yifan Liu</a>,
              <a href="https://www.flinders.edu.au/people/minhson.to">Minh-Son To</a><sup>✉</sup>
              <br><div class="spacer"></div>
              <a href="https://doi.org/10.1007/978-3-031-45673-2_15"><img src="https://img.shields.io/badge/DOI-10.1007%2F978--3--031--45673--2__15-fcb520?style=flat-square&logo=doi"></a>
              <a href="https://arxiv.org/abs/2308.11298"><img src="https://img.shields.io/badge/arXiv-2308.11298-b31b1b?style=flat-square&logo=arxiv"></a>
              <a href="https://openreview.net/forum?id=R5951Pk_vlw"><img src="https://img.shields.io/badge/OpenReview-8c1b13?style=flat-square"></a>
              <a href="https://github.com/White65534/BHSD"><img src="https://img.shields.io/badge/GitHub-Code-1f883d?style=flat-square&logo=github"></a>
              <a href="https://paperswithcode.com/paper/bhsd-a-3d-multi-class-brain-hemorrhage"><img src="https://img.shields.io/badge/Papers%20With%20Code-555555.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://paperswithcode.com/dataset/bhsd"><img src="https://img.shields.io/badge/Papers%20With%20Code-Dataset-21cbce.svg?style=flat-square&logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNTEyIDUxMiIgd2lkdGg9IjUxMiIgIGhlaWdodD0iNTEyIiA+PHBhdGggZD0iTTg4IDEyOGg0OHYyNTZIODh6bTE0NCAwaDQ4djI1NmgtNDh6bS03MiAxNmg0OHYyMjRoLTQ4em0xNDQgMGg0OHYyMjRoLTQ4em03Mi0xNmg0OHYyNTZoLTQ4eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PHBhdGggZD0iTTEwNCAxMDRWNTZIMTZ2NDAwaDg4di00OEg2NFYxMDR6bTMwNC00OHY0OGg0MHYzMDRoLTQwdjQ4aDg4VjU2eiIgc3Ryb2tlPSIjMjFDQkNFIiBmaWxsPSIjMjFDQkNFIj48L3BhdGg+PC9zdmc+"></a>
              <a href="https://github.com/White65534/BHSD/tree/main#citation"><img src="https://img.shields.io/badge/BibTeX-Citation-eeeeee?style=flat-square"></a>
              <br>
              <a href="https://sites.google.com/view/mlmi2023"><strong><em>MLMI 2023</em></strong><br></a>
              The Brain Hemorrhage Segmentation Dataset (BHSD) is a comprehensive 3D multi-class ICH dataset with pixel-level and slice-level annotations designed to support supervised and semi-supervised ICH segmentation tasks, addressing the lack of existing public datasets for multi-class ICH segmentation.
            </td>
          </tr>

        </tbody></table>
      </div></details>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Research Experience</h2>
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/csail.png" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Visiting Student Researcher</papertitle>
              <br>
              <a href="https://www.csail.mit.edu/">MIT CSAIL</a>
              <br>
              <em>Jun 2024 - Present</em>
              <br>
              Worked on physically compatible 3D generation in <a href="https://hcie.csail.mit.edu/">MIT CSAIL HCIE group</a>, working with <a href="https://hcie.csail.mit.edu/stefanie-mueller.html">Assoc. Prof. Stefanie Mueller</a> (MIT CSAIL) and <a href="https://sites.mit.edu/farazfaruqi/">Mr. Faraz Faruqi</a> (MIT CSAIL).
              </td>
            </tr>


            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/giga.png" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Intern</papertitle>
              <br>
              <a href="https://gigaai.cc/">Giga AI</a>
              <br>
              <em>Dec 2024 - Present</em>
              <br>
              3D generation, spatial intelligence, and world model, working with <a href="http://www.zhengzhu.net/">Dr. Zheng Zhu</a> (GigaAI).
              </td>
            </tr>


            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/damo.png" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Intern</papertitle>
              <br>
              <a href="https://damo.alibaba.com/">Alibaba DAMO Academy</a>
              <br>
              <em>Oct 2024 - Present</em>
              <br>
              Efficient long video generation, working with Mr. Jiasheng Tang (DAMO) and <a href="https://bohanzhuang.github.io/">Prof. Bohan Zhuang</a> (ZJU, DAMO).
              </td>
            </tr>


            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/zju.png" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Assistant</papertitle>
              <br>
              <a href="https://www.zju.edu.cn/">Zhejiang University</a>
              <br>
              <em>Aug 2024 - Present</em>
              <br>
              Worked on efficient generative models, working with <a href="https://bohanzhuang.github.io/">Prof. Bohan Zhuang</a> (ZJU).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/pku.png" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Visiting Student Researcher</papertitle>
              <br>
              <a href="https://english.pku.edu.cn/">Peking University</a>
              <br>
              <em>July 2024 - Present</em>
              <br>
              Worked on 3D human motion generation, working with <a href="https://ha0tang.github.io/">Asst. Prof. Hao Tang</a> (PKU).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/mbzuai.jpg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Intern</papertitle>
              <br>
              <a href="https://mbzuai.ac.ae/">Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)</a>
              <br>
              <em>May 2024 - June 2024</em>
              <br>
              Worked on unsupervised classification of cellular structures based on cryo-electron tomography (cryo-ET), working with <a href="https://xulabs.github.io/min-xu/">Assoc. Prof. Min Xu</a> (CMU, MBZUAI) and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a> (MBZUAI, AIML).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/latrobe.jpeg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Assistant</papertitle>
              <br>
              <a href="https://www.latrobe.edu.au/">La Trobe University</a>
              <br>
              <em>Apr 2024 - Present</em>
              <br>
              Worked on 3D generation and AI for Heath, working with <a href="https://yangyangkiki.github.io/">Dr. Yang Zhao</a> (La Trobe University).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/monash.jpeg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Assistant</papertitle>
              <br>
              <a href="https://www.monash.edu/">Monash University</a>
              <br>
              <em>Feb 2024 - May 2024</em>
              <br>
              Worked on 3D/4D generative learning, specifically focusing on text-guided human motion and avatar generation, working with <a href="https://users.monash.edu.au/~gholamrh/">Prof. Reza Haffari</a> (Monash University), and <a href="https://bohanzhuang.github.io/">Prof. Bohan Zhuang</a> (ZJU, Monash University).
              </td>
            </tr>


            <tr onmouseout="ld_stop()" onmouseover="ld_start()"></tr>
              <td style="padding:0px;vertical-align:middle">
                <img src="image/nci.jpeg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Intern</papertitle>
              <br>
              <a href="https://nci.org.au/">National Computational Infrastructure (NCI)</a>
              <br>
              <em>Feb 2023 - Jun 2023</em>
              <br>
              Worked on long tail large scale multi-label text classification, working with <a href="https://nci.org.au/research/people/dr-jingbo-wang">Dr. Jingbo Wang</a> (NCI).
              </td>
            </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/aiml.jpg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Visiting Student Researcher</papertitle>
              <br>
              <a href="https://www.adelaide.edu.au/aiml/">Australian Institute for Machine Learning (AIML)</a>
              <br>
              <em>Nov 2022 - Jan 2024</em>
              <br>
              Worked on 3D medical imaging analysis, with a particular focus on semantic segmentations of tumors, hemorrhages, and organs at risk, working with <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian Reid</a> (MBZUAI, AIML), <a href="https://www.adelaide.edu.au/directory/b.zhang">Dr. Bowen Zhang</a> (AIML), <a href="https://researchers.adelaide.edu.au/profile/yutong.xie">Dr. Yutong Xie</a> (AIML), and <a href="https://scholar.google.com/citations?user=OgKU77kAAAAJ&hl=zh-CN">Dr. Qi Chen</a> (AIML).
              </td>
            </tr>


            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/fhmri.png" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Research Assistant</papertitle>
              <br>
              <a href="https://www.flinders.edu.au/health-medical-research-institute">Flinders Health and Medical Research Institute (FHMRI)</a>
              <br>
              <em>Nov 2022 - Present</em>
              <br>
              Worked on 3D medical imaging analysis, particularly in the realms of 2D and 3D medical representation learning and explainable AI, working with <a href="https://scholar.google.com/citations?user=NIc4qPsAAAAJ&hl=en">Dr. Minh-Son To</a> (FHMRI).
              </td>
            </tr>


            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;vertical-align:middle">
                <img src="image/anu.jpg" width="50" style="border-style: none">
              </td>
              <td style="padding:10px;vertical-align:middle">
                <papertitle>Student Researcher</papertitle>
              <br>
              <a href="https://www.anu.edu.au/">The Australian National University (ANU)</a>
              <br>
              <em>Jul 2022 - Nov 2022</em>
              <br>
              Worked on diabetes diagnosis in deep learning, working with <a href="https://radiopaedia.org/users/kaspar-lewis-yaxley?lang=us">Dr. Md Zakir Hossain</a> (ANU, Curtin University, CSIRO Data61), <a href="https://scholar.google.com/citations?user=rUJ9DVAAAAAJ&hl=en">Dr. Khandaker Asif Ahmed</a> (CSIRO), <a href="https://staffportal.curtin.edu.au/staff/profile/view/rakibul-hasan-145a1046/"> Mr. Md Rakibul Hasan</a> (Curtin University, Brac University), and <a href="https://staffportal.curtin.edu.au/staff/profile/view/tom-gedeon-5e48a1fd/">Prof. Tom Gedeon</a> (Curtin University, ANU, Óbuda University).
              </td>
            </tr>  

          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Education</h2>
            </td>
          </tr>
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;vertical-align:middle">
              <img src="image/anu.jpg" width="50" style="border-style: none">
            </td>
            <td style="padding:10px;vertical-align:middle;width:85%;">
              <papertitle>Bachelor of Science (Advanced) (Honours)</papertitle>
            <br>
            <a href="https://www.anu.edu.au/">The Australian National University (ANU)</a>
            <br>
            <em>Jul 2021 - Jun 2025 (Expected)</em>
            <br>
            Major: Computer Science, Minor: Mathematics
            </td>
          </tr>  
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;vertical-align:middle">
              <img src="image/ic.jpg" width="50" style="border-style: none">
            </td>
            <td style="padding:10px;vertical-align:middle;width:85%;">
              <papertitle>Visiting Student</papertitle>
            <br>
            <a href="https://www.imperial.ac.uk/">Imperial College London</a>
            <br>
            <em>Jul 2022</em>
            <br>
            Quantitative Sciences Research Institute (QSRI)
            </td>
          </tr>  
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;vertical-align:middle">
              <img src="image/ucl.svg" width="50" style="border-style: none">
            </td>
            <td style="padding:10px;vertical-align:middle;width:85%;">
              <papertitle>Visiting Student</papertitle>
            <br>
            <a href="https://www.ucl.ac.uk/">University College London (UCL)</a>
            <br>
            <em>Jul 2022</em>
            <br>
            </td>
          </tr>  
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;vertical-align:middle">
              <img src="image/sjtu.jpeg" width="50" style="border-style: none">
            </td>
            <td style="padding:10px;vertical-align:middle;width:85%;">
              <papertitle>Visiting Student</papertitle>
            <br>
            <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>
            <br>
            <em>Dec 2021 - Jan 2022</em>
            <br>
            </td>
          </tr>  
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Honors & Awards</h2>
          </td>
        </tr>
      </tbody></table>

      
      <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
    </tbody></table>

    <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr onmouseout="ld_stop()" onmouseover="ld_start()">
        <td style="padding:10px;vertical-align:middle">
          <a href="https://www.nrf.com.au/news/nrf-vacation-scholarships-2023"><b>NRF Vacation Scholarship</b></a>, NeuroSurgical Research Foundation, Oct 2023.<br>
          <a href="https://www.flinders.edu.au/scholarships/college-of-medicine-and-public-health-summer-research-scholarship"><b>Flinders Summer Research Scholarship</b></a>, Flinders University CMPH, Nov 2022.<br>
          <a href="https://www.scholarships.unsw.edu.au/scholarships/id/1252"><b>UNSW Science Vacation Research Scholarship</b></a>, The UNSW Sydney, Oct 2022.<br>
        </td>
      </tr>
    </tbody></table>  






          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Academic Services</h2>
              </td>
            </tr>
          </tbody></table>

          
          <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	      
        </tbody></table>

        <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:10px;vertical-align:middle">
              <b>Conference:</b> <a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>, <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a>, <a href="https://2025.ijcai.org/">IJCAI 2025</a>, <a href="https://chi2025.acm.org/">CHI 2025</a>, <a href="https://ieeevr.org/2025/">VR 2025</a>, <a href="https://2025.ieeeicme.org/">ICME 2025</a>, <a href="https://2025.ieeeicassp.org/">ICASSP 2025</a>, <a href="https://2025.ijcnn.org/">IJCNN 2025</a>, <a href="https://biomedicalimaging.org/2025/">ISBI 2025</a>, <a href="https://miua2024.github.io/">MIUA 2024</a>, <a href="https://ieeebibm.org/BIBM2024/">BIBM 2024</a>.<br>
              <b>Journal:</b> <a href="https://www.ebm-journal.org/journals/experimental-biology-and-medicine">EBM</a>, <a href="https://www.scientificarchives.com/journal/Archives-of-Clinical-Ophthalmology">ACO</a>, <a href="https://themedicon.com/engineeringthemes">MCET</a>, <a href="https://primerascientific.com/psen">PSEN</a>.<br>
            </td>
          </tr>
        </tbody></table>  







          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Talks</h2>
              </td>
            </tr>
          </tbody></table>

          
          <table style="width:75%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:10px;vertical-align:middle">
                <b>(07/22/2024)</b> <a href="https://steve-zeyu-zhang.github.io/MotionMamba/"><b>Motion Mamba: Efficient and Long Sequence Motion Generation</b></a> @ <a href="https://www.mihoyo.com/"><b>miHoYo</b></a>, Shanghai. You can find our slides <a href="https://steve-zeyu-zhang.github.io/MotionMamba/static/pdfs/Motion_Mamba_Slides_miHoYo.pdf"><b>here</b></a>.
              </td>
            </tr>
          </tbody></table>        


        <p style="height: 40px;">&nbsp;</p><br>
      </td>
    </tr>
  </table>
</body>

</html>
<!--for page update--> 
